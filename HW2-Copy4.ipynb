{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets improve libraries that we are going to be used in this lab session\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(134)\n",
    "import pandas as pd\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('hw2_data/snli_train.tsv',delimiter='\\t',encoding='utf-8')\n",
    "val=pd.read_csv('hw2_data/snli_val.tsv',delimiter='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets=list(train['label'])\n",
    "val_targets=list(val['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_targets= [0 if x=='entailment' else x for x in val_targets]\n",
    "train_targets= [0 if x=='entailment' else x for x in train_targets]\n",
    "val_targets= [1 if x=='neutral' else x for x in val_targets]\n",
    "train_targets= [1 if x=='neutral' else x for x in train_targets]\n",
    "val_targets= [2 if x=='contradiction' else x for x in val_targets]\n",
    "train_targets= [2 if x=='contradiction' else x for x in train_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sen1= list(train['sentence1'])\n",
    "train_sen2= list(train['sentence2'])\n",
    "val_sen1= list(val['sentence1'])\n",
    "val_sen2= list(val['sentence2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "b=[]\n",
    "for i in range(0,100000):\n",
    "    tmp1=''\n",
    "    tmp2=''\n",
    "    for j in train_sen1[i].split(' '):\n",
    "        tmp1 =tmp1 + ' ' + j\n",
    "    for j in train_sen2[i].split(' '):\n",
    "        tmp2=tmp2 + ' ' + j\n",
    "    a.append(tmp1[1:])\n",
    "    b.append(tmp2[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sen1= a\n",
    "train_sen2= b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "b=[]\n",
    "for i in range(0,1000):\n",
    "    tmp1=''\n",
    "    tmp2=''\n",
    "    for j in val_sen1[i].split(' '):\n",
    "        tmp1 =tmp1 + ' ' + j\n",
    "    for j in val_sen2[i].split(' '):\n",
    "        tmp2=tmp2 + ' ' + j\n",
    "    a.append(tmp1[1:])\n",
    "    b.append(tmp2[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sen1= a\n",
    "val_sen2= b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize(sent):\n",
    "  tokens = tokenizer(sent)\n",
    "  return [token.text for token in tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        \n",
    "        token_dataset.append(tokens)\n",
    "        \n",
    "    return token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_sen1=tokenize_dataset(train_sen1)\n",
    "tt_sen2=tokenize_dataset(train_sen2)\n",
    "tv_sen1=tokenize_dataset(val_sen1)\n",
    "tv_sen2=tokenize_dataset(val_sen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(tt_sen1, open(\"tt_sen1.p\", \"wb\"))\n",
    "pkl.dump(tt_sen2, open(\"tt_sen2.p\", \"wb\"))\n",
    "pkl.dump(tv_sen1, open(\"tv_sen1.p\", \"wb\"))\n",
    "pkl.dump(tv_sen2, open(\"tv_sen2.p\", \"wb\"))\n",
    "pkl.dump(train_targets, open(\"train_targets.p\", \"wb\"))\n",
    "pkl.dump(val_targets, open(\"val_targets.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "tt_sen1 = pkl.load(open(\"tt_sen1.p\", \"rb\"))\n",
    "tt_sen2 = pkl.load(open(\"tt_sen2.p\", \"rb\"))\n",
    "tv_sen1 = pkl.load(open(\"tv_sen1.p\", \"rb\"))\n",
    "tv_sen2 = pkl.load(open(\"tv_sen2.p\", \"rb\"))\n",
    "train_targets = pkl.load(open(\"train_targets.p\", \"rb\"))\n",
    "val_targets = pkl.load(open(\"val_targets.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_load = 50000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open('wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i\n",
    "        idx2words_ft[i] = s[0]\n",
    "        ordered_words_ft.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id2token\n",
    "x=idx2words_ft\n",
    "#token2idx\n",
    "y=words_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2words_ft={}\n",
    "idx2words_ft[0]='<pad>'\n",
    "idx2words_ft[1]='<unk>'\n",
    "for i in range(0,50000):\n",
    "    idx2words_ft[i+2]=x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=list(words_ft.keys())\n",
    "words_ft={}\n",
    "words_ft['<pad>']=0\n",
    "words_ft['<unk>']=1\n",
    "for i in range(0,50000):\n",
    "    words_ft[w[i]]=i+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=loaded_embeddings_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings_ft=np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings_ft=np.vstack([loaded_embeddings_ft,np.ones(300)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings_ft=np.vstack([loaded_embeddings_ft,z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id=words_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token=list(idx2words_ft.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 100000\n",
      "Train dataset size is 100000\n",
      "Val dataset size is 1000\n",
      "Val dataset size is 1000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices1 = token2index_dataset(tt_sen1)\n",
    "train_data_indices2 = token2index_dataset(tt_sen2)\n",
    "val_data_indices1 = token2index_dataset(tv_sen1)\n",
    "val_data_indices2 = token2index_dataset(tv_sen2)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices1)))\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices2)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices1)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm=0\n",
    "for i in range(0,100000):\n",
    "    r=max(len(train_data_indices1[i]),len(train_data_indices2[i]))\n",
    "    if mm<r:\n",
    "        mm=r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"    \n",
    "    def __init__(self, data_list1,data_list2, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1 = data_list1\n",
    "        self.data_list2 = data_list2\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list1) == len(self.target_list))\n",
    "        assert (len(self.data_list2) == len(self.target_list))\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "    def __len__(self):\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "        return len(self.data_list1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        token_idx1 = self.data_list1[key][:MAX_SENTENCE_LENGTH]       \n",
    "        token_idx2 = self.data_list2[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx1, len(token_idx1),token_idx2, len(token_idx2), label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = mm\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"    \n",
    "    def __init__(self, data_list1,data_list2, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1 = data_list1\n",
    "        self.data_list2 = data_list2\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list1) == len(self.target_list))\n",
    "        assert (len(self.data_list2) == len(self.target_list))\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "    def __len__(self):\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "        return len(self.data_list1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        token_idx1 = self.data_list1[key][:MAX_SENTENCE_LENGTH]       \n",
    "        token_idx2 = self.data_list2[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx1, len(token_idx1),token_idx2, len(token_idx2), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    length_list1 = []\n",
    "    data_list2 = []\n",
    "    length_list2 = []\n",
    "    label_list = []\n",
    "\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list1.append(datum[1])\n",
    "        length_list2.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list1.append(padded_vec1)\n",
    "        padded_vec2 = np.pad(np.array(datum[2]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list2.append(padded_vec2)\n",
    "        \n",
    "#     ind_dec_order1 = np.argsort(length_list1)[::-1]\n",
    "#     ind_dec_order2 = np.argsort(length_list2)[::-1]\n",
    "    \n",
    "#     data_list1 = np.array(data_list1)[ind_dec_order1]\n",
    "#     data_list2 = np.array(data_list2)[ind_dec_order2]\n",
    "    \n",
    "#     length_list1 = np.array(length_list1)[ind_dec_order1]\n",
    "#     length_list2 = np.array(length_list2)[ind_dec_order2]\n",
    "    \n",
    "#     label_list = np.array(label_list)[ind_dec_order]    \n",
    "        \n",
    "    return [torch.from_numpy(np.array(data_list1)), torch.from_numpy(np.array(np.int32(length_list1))) ,torch.from_numpy(np.array(data_list2)), torch.from_numpy(np.array(np.int32(length_list2))), torch.from_numpy(np.array(np.int32(label_list))).long()]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices1, train_data_indices2, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices1, val_data_indices2, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets implement basic Convolutional Neural Net model for text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.FloatTensor(loaded_embeddings_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight,freeze=True)\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "\n",
    "        self.linear = nn.Linear(2*hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x1, lengths1,x2, lengths2):\n",
    "        batch_size1, seq_len1 = x1.size()\n",
    "        batch_size2, seq_len2 = x2.size()\n",
    "        \n",
    "        embed1 = self.embedding(x1)\n",
    "        embed2 = self.embedding(x2)        \n",
    "        #First sentence\n",
    "        hidden1 = self.conv1(embed1.transpose(1,2)).transpose(1,2)\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size1, seq_len1, hidden1.size(-1))\n",
    "\n",
    "        hidden1 = self.conv2(hidden1.transpose(1,2)).transpose(1,2)\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size1, seq_len1, hidden1.size(-1))\n",
    "        hidden1 = torch.sum(hidden1, dim=1)\n",
    "       \n",
    "        \n",
    "        #second sentence\n",
    "        hidden2 = self.conv1(embed2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size2, seq_len2, hidden2.size(-1))\n",
    "\n",
    "        hidden2 = self.conv2(hidden2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size2, seq_len2, hidden2.size(-1))\n",
    "        hidden2 = torch.sum(hidden2, dim=1)\n",
    "        \n",
    "        hiddenfinal = torch.cat((hidden1, hidden2), 1)\n",
    "        logits = self.linear(hiddenfinal)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 46.7\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 53.9\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 55.9\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 59.2\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 58.8\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 57.1\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 57.9\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 60.9\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 59.9\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 59.2\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 60.2\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 59.8\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 58.9\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 60.1\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 61.2\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 62.3\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 62.9\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 62.1\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 61.5\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 62.9\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 62.3\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 61.9\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 62.0\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 62.6\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 62.7\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 63.3\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 61.8\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 61.2\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 61.7\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 62.6\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 61.4\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 62.8\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 62.6\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 61.7\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 63.4\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 63.7\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 64.1\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 63.6\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 63.2\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 63.1\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 62.8\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 64.2\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 63.6\n",
      "Epoch: [2/10], Step: [1301/3125], Validation Acc: 64.0\n",
      "Epoch: [2/10], Step: [1401/3125], Validation Acc: 62.7\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 62.4\n",
      "Epoch: [2/10], Step: [1601/3125], Validation Acc: 63.0\n",
      "Epoch: [2/10], Step: [1701/3125], Validation Acc: 62.9\n",
      "Epoch: [2/10], Step: [1801/3125], Validation Acc: 63.6\n",
      "Epoch: [2/10], Step: [1901/3125], Validation Acc: 63.8\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 62.5\n",
      "Epoch: [2/10], Step: [2101/3125], Validation Acc: 64.3\n",
      "Epoch: [2/10], Step: [2201/3125], Validation Acc: 64.7\n",
      "Epoch: [2/10], Step: [2301/3125], Validation Acc: 63.7\n",
      "Epoch: [2/10], Step: [2401/3125], Validation Acc: 64.6\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 63.5\n",
      "Epoch: [2/10], Step: [2601/3125], Validation Acc: 65.0\n",
      "Epoch: [2/10], Step: [2701/3125], Validation Acc: 65.1\n",
      "Epoch: [2/10], Step: [2801/3125], Validation Acc: 64.3\n",
      "Epoch: [2/10], Step: [2901/3125], Validation Acc: 65.3\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 63.0\n",
      "Epoch: [2/10], Step: [3101/3125], Validation Acc: 65.1\n",
      "Epoch: [3/10], Step: [101/3125], Validation Acc: 64.4\n",
      "Epoch: [3/10], Step: [201/3125], Validation Acc: 63.9\n",
      "Epoch: [3/10], Step: [301/3125], Validation Acc: 64.5\n",
      "Epoch: [3/10], Step: [401/3125], Validation Acc: 64.9\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 65.2\n",
      "Epoch: [3/10], Step: [601/3125], Validation Acc: 64.7\n",
      "Epoch: [3/10], Step: [701/3125], Validation Acc: 66.3\n",
      "Epoch: [3/10], Step: [801/3125], Validation Acc: 63.6\n",
      "Epoch: [3/10], Step: [901/3125], Validation Acc: 64.6\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 64.7\n",
      "Epoch: [3/10], Step: [1101/3125], Validation Acc: 64.9\n",
      "Epoch: [3/10], Step: [1201/3125], Validation Acc: 65.7\n",
      "Epoch: [3/10], Step: [1301/3125], Validation Acc: 64.1\n",
      "Epoch: [3/10], Step: [1401/3125], Validation Acc: 61.8\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 64.4\n",
      "Epoch: [3/10], Step: [1601/3125], Validation Acc: 65.0\n",
      "Epoch: [3/10], Step: [1701/3125], Validation Acc: 64.4\n",
      "Epoch: [3/10], Step: [1801/3125], Validation Acc: 63.2\n",
      "Epoch: [3/10], Step: [1901/3125], Validation Acc: 64.6\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 61.8\n",
      "Epoch: [3/10], Step: [2101/3125], Validation Acc: 62.0\n",
      "Epoch: [3/10], Step: [2201/3125], Validation Acc: 64.5\n",
      "Epoch: [3/10], Step: [2301/3125], Validation Acc: 65.7\n",
      "Epoch: [3/10], Step: [2401/3125], Validation Acc: 63.8\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 64.4\n",
      "Epoch: [3/10], Step: [2601/3125], Validation Acc: 64.0\n",
      "Epoch: [3/10], Step: [2701/3125], Validation Acc: 64.8\n",
      "Epoch: [3/10], Step: [2801/3125], Validation Acc: 64.1\n",
      "Epoch: [3/10], Step: [2901/3125], Validation Acc: 65.1\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 65.5\n",
      "Epoch: [3/10], Step: [3101/3125], Validation Acc: 64.7\n",
      "Epoch: [4/10], Step: [101/3125], Validation Acc: 65.4\n",
      "Epoch: [4/10], Step: [201/3125], Validation Acc: 65.6\n",
      "Epoch: [4/10], Step: [301/3125], Validation Acc: 64.4\n",
      "Epoch: [4/10], Step: [401/3125], Validation Acc: 64.2\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 63.8\n",
      "Epoch: [4/10], Step: [601/3125], Validation Acc: 63.8\n",
      "Epoch: [4/10], Step: [701/3125], Validation Acc: 64.0\n",
      "Epoch: [4/10], Step: [801/3125], Validation Acc: 65.4\n",
      "Epoch: [4/10], Step: [901/3125], Validation Acc: 63.8\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 64.0\n",
      "Epoch: [4/10], Step: [1101/3125], Validation Acc: 65.1\n",
      "Epoch: [4/10], Step: [1201/3125], Validation Acc: 64.6\n",
      "Epoch: [4/10], Step: [1301/3125], Validation Acc: 66.7\n",
      "Epoch: [4/10], Step: [1401/3125], Validation Acc: 64.4\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 62.8\n",
      "Epoch: [4/10], Step: [1601/3125], Validation Acc: 64.4\n",
      "Epoch: [4/10], Step: [1701/3125], Validation Acc: 64.2\n",
      "Epoch: [4/10], Step: [1801/3125], Validation Acc: 65.0\n",
      "Epoch: [4/10], Step: [1901/3125], Validation Acc: 64.5\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 64.2\n",
      "Epoch: [4/10], Step: [2101/3125], Validation Acc: 63.9\n",
      "Epoch: [4/10], Step: [2201/3125], Validation Acc: 64.2\n",
      "Epoch: [4/10], Step: [2301/3125], Validation Acc: 61.6\n",
      "Epoch: [4/10], Step: [2401/3125], Validation Acc: 63.0\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 64.3\n",
      "Epoch: [4/10], Step: [2601/3125], Validation Acc: 64.8\n",
      "Epoch: [4/10], Step: [2701/3125], Validation Acc: 65.2\n",
      "Epoch: [4/10], Step: [2801/3125], Validation Acc: 64.4\n",
      "Epoch: [4/10], Step: [2901/3125], Validation Acc: 63.8\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 63.5\n",
      "Epoch: [4/10], Step: [3101/3125], Validation Acc: 65.3\n",
      "Epoch: [5/10], Step: [101/3125], Validation Acc: 63.6\n",
      "Epoch: [5/10], Step: [201/3125], Validation Acc: 64.6\n",
      "Epoch: [5/10], Step: [301/3125], Validation Acc: 64.7\n",
      "Epoch: [5/10], Step: [401/3125], Validation Acc: 62.6\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 64.2\n",
      "Epoch: [5/10], Step: [601/3125], Validation Acc: 65.3\n",
      "Epoch: [5/10], Step: [701/3125], Validation Acc: 64.6\n",
      "Epoch: [5/10], Step: [801/3125], Validation Acc: 64.2\n",
      "Epoch: [5/10], Step: [901/3125], Validation Acc: 63.8\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 64.3\n",
      "Epoch: [5/10], Step: [1101/3125], Validation Acc: 63.8\n",
      "Epoch: [5/10], Step: [1201/3125], Validation Acc: 64.5\n",
      "Epoch: [5/10], Step: [1301/3125], Validation Acc: 64.7\n",
      "Epoch: [5/10], Step: [1401/3125], Validation Acc: 65.8\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 62.9\n",
      "Epoch: [5/10], Step: [1601/3125], Validation Acc: 64.7\n",
      "Epoch: [5/10], Step: [1701/3125], Validation Acc: 64.5\n",
      "Epoch: [5/10], Step: [1801/3125], Validation Acc: 64.0\n",
      "Epoch: [5/10], Step: [1901/3125], Validation Acc: 63.9\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 64.5\n",
      "Epoch: [5/10], Step: [2101/3125], Validation Acc: 65.5\n",
      "Epoch: [5/10], Step: [2201/3125], Validation Acc: 64.7\n",
      "Epoch: [5/10], Step: [2301/3125], Validation Acc: 64.5\n",
      "Epoch: [5/10], Step: [2401/3125], Validation Acc: 65.5\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 64.4\n",
      "Epoch: [5/10], Step: [2601/3125], Validation Acc: 64.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/10], Step: [2701/3125], Validation Acc: 64.8\n",
      "Epoch: [5/10], Step: [2801/3125], Validation Acc: 63.4\n",
      "Epoch: [5/10], Step: [2901/3125], Validation Acc: 63.4\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 63.8\n",
      "Epoch: [5/10], Step: [3101/3125], Validation Acc: 63.3\n",
      "Epoch: [6/10], Step: [101/3125], Validation Acc: 64.1\n",
      "Epoch: [6/10], Step: [201/3125], Validation Acc: 64.9\n",
      "Epoch: [6/10], Step: [301/3125], Validation Acc: 63.8\n",
      "Epoch: [6/10], Step: [401/3125], Validation Acc: 61.6\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 62.0\n",
      "Epoch: [6/10], Step: [601/3125], Validation Acc: 63.2\n",
      "Epoch: [6/10], Step: [701/3125], Validation Acc: 62.9\n",
      "Epoch: [6/10], Step: [801/3125], Validation Acc: 62.8\n",
      "Epoch: [6/10], Step: [901/3125], Validation Acc: 64.7\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 64.7\n",
      "Epoch: [6/10], Step: [1101/3125], Validation Acc: 61.7\n",
      "Epoch: [6/10], Step: [1201/3125], Validation Acc: 62.4\n",
      "Epoch: [6/10], Step: [1301/3125], Validation Acc: 63.8\n",
      "Epoch: [6/10], Step: [1401/3125], Validation Acc: 63.5\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 63.8\n",
      "Epoch: [6/10], Step: [1601/3125], Validation Acc: 63.8\n",
      "Epoch: [6/10], Step: [1701/3125], Validation Acc: 63.8\n",
      "Epoch: [6/10], Step: [1801/3125], Validation Acc: 63.6\n",
      "Epoch: [6/10], Step: [1901/3125], Validation Acc: 63.0\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 63.8\n",
      "Epoch: [6/10], Step: [2101/3125], Validation Acc: 63.6\n",
      "Epoch: [6/10], Step: [2201/3125], Validation Acc: 64.8\n",
      "Epoch: [6/10], Step: [2301/3125], Validation Acc: 63.0\n",
      "Epoch: [6/10], Step: [2401/3125], Validation Acc: 61.5\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 62.0\n",
      "Epoch: [6/10], Step: [2601/3125], Validation Acc: 62.7\n",
      "Epoch: [6/10], Step: [2701/3125], Validation Acc: 64.8\n",
      "Epoch: [6/10], Step: [2801/3125], Validation Acc: 64.6\n",
      "Epoch: [6/10], Step: [2901/3125], Validation Acc: 64.4\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 65.5\n",
      "Epoch: [6/10], Step: [3101/3125], Validation Acc: 63.4\n",
      "Epoch: [7/10], Step: [101/3125], Validation Acc: 64.7\n",
      "Epoch: [7/10], Step: [201/3125], Validation Acc: 64.0\n",
      "Epoch: [7/10], Step: [301/3125], Validation Acc: 63.2\n",
      "Epoch: [7/10], Step: [401/3125], Validation Acc: 63.8\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 63.7\n",
      "Epoch: [7/10], Step: [601/3125], Validation Acc: 63.4\n",
      "Epoch: [7/10], Step: [701/3125], Validation Acc: 63.7\n",
      "Epoch: [7/10], Step: [801/3125], Validation Acc: 64.3\n",
      "Epoch: [7/10], Step: [901/3125], Validation Acc: 63.6\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 63.6\n",
      "Epoch: [7/10], Step: [1101/3125], Validation Acc: 64.3\n",
      "Epoch: [7/10], Step: [1201/3125], Validation Acc: 63.9\n",
      "Epoch: [7/10], Step: [1301/3125], Validation Acc: 63.6\n",
      "Epoch: [7/10], Step: [1401/3125], Validation Acc: 62.5\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 63.3\n",
      "Epoch: [7/10], Step: [1601/3125], Validation Acc: 61.9\n",
      "Epoch: [7/10], Step: [1701/3125], Validation Acc: 61.7\n",
      "Epoch: [7/10], Step: [1801/3125], Validation Acc: 62.3\n",
      "Epoch: [7/10], Step: [1901/3125], Validation Acc: 63.7\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 64.1\n",
      "Epoch: [7/10], Step: [2101/3125], Validation Acc: 63.6\n",
      "Epoch: [7/10], Step: [2201/3125], Validation Acc: 62.3\n",
      "Epoch: [7/10], Step: [2301/3125], Validation Acc: 63.0\n",
      "Epoch: [7/10], Step: [2401/3125], Validation Acc: 62.2\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 62.7\n",
      "Epoch: [7/10], Step: [2601/3125], Validation Acc: 63.7\n",
      "Epoch: [7/10], Step: [2701/3125], Validation Acc: 62.4\n",
      "Epoch: [7/10], Step: [2801/3125], Validation Acc: 63.2\n",
      "Epoch: [7/10], Step: [2901/3125], Validation Acc: 63.0\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 62.7\n",
      "Epoch: [7/10], Step: [3101/3125], Validation Acc: 63.3\n",
      "Epoch: [8/10], Step: [101/3125], Validation Acc: 63.3\n",
      "Epoch: [8/10], Step: [201/3125], Validation Acc: 65.1\n",
      "Epoch: [8/10], Step: [301/3125], Validation Acc: 63.8\n",
      "Epoch: [8/10], Step: [401/3125], Validation Acc: 63.7\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 62.9\n",
      "Epoch: [8/10], Step: [601/3125], Validation Acc: 63.6\n",
      "Epoch: [8/10], Step: [701/3125], Validation Acc: 63.6\n",
      "Epoch: [8/10], Step: [801/3125], Validation Acc: 62.5\n",
      "Epoch: [8/10], Step: [901/3125], Validation Acc: 62.8\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 63.5\n",
      "Epoch: [8/10], Step: [1101/3125], Validation Acc: 64.0\n",
      "Epoch: [8/10], Step: [1201/3125], Validation Acc: 62.3\n",
      "Epoch: [8/10], Step: [1301/3125], Validation Acc: 61.7\n",
      "Epoch: [8/10], Step: [1401/3125], Validation Acc: 61.9\n",
      "Epoch: [8/10], Step: [1501/3125], Validation Acc: 62.3\n",
      "Epoch: [8/10], Step: [1601/3125], Validation Acc: 62.6\n",
      "Epoch: [8/10], Step: [1701/3125], Validation Acc: 61.9\n",
      "Epoch: [8/10], Step: [1801/3125], Validation Acc: 62.4\n",
      "Epoch: [8/10], Step: [1901/3125], Validation Acc: 63.2\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 63.2\n",
      "Epoch: [8/10], Step: [2101/3125], Validation Acc: 63.8\n",
      "Epoch: [8/10], Step: [2201/3125], Validation Acc: 62.8\n",
      "Epoch: [8/10], Step: [2301/3125], Validation Acc: 63.5\n",
      "Epoch: [8/10], Step: [2401/3125], Validation Acc: 64.7\n",
      "Epoch: [8/10], Step: [2501/3125], Validation Acc: 62.1\n",
      "Epoch: [8/10], Step: [2601/3125], Validation Acc: 63.4\n",
      "Epoch: [8/10], Step: [2701/3125], Validation Acc: 62.8\n",
      "Epoch: [8/10], Step: [2801/3125], Validation Acc: 63.0\n",
      "Epoch: [8/10], Step: [2901/3125], Validation Acc: 61.1\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 62.9\n",
      "Epoch: [8/10], Step: [3101/3125], Validation Acc: 62.8\n",
      "Epoch: [9/10], Step: [101/3125], Validation Acc: 62.8\n",
      "Epoch: [9/10], Step: [201/3125], Validation Acc: 62.2\n",
      "Epoch: [9/10], Step: [301/3125], Validation Acc: 62.3\n",
      "Epoch: [9/10], Step: [401/3125], Validation Acc: 61.0\n",
      "Epoch: [9/10], Step: [501/3125], Validation Acc: 62.5\n",
      "Epoch: [9/10], Step: [601/3125], Validation Acc: 62.3\n",
      "Epoch: [9/10], Step: [701/3125], Validation Acc: 62.9\n",
      "Epoch: [9/10], Step: [801/3125], Validation Acc: 61.0\n",
      "Epoch: [9/10], Step: [901/3125], Validation Acc: 62.3\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 61.7\n",
      "Epoch: [9/10], Step: [1101/3125], Validation Acc: 62.7\n",
      "Epoch: [9/10], Step: [1201/3125], Validation Acc: 62.4\n",
      "Epoch: [9/10], Step: [1301/3125], Validation Acc: 62.8\n",
      "Epoch: [9/10], Step: [1401/3125], Validation Acc: 63.0\n",
      "Epoch: [9/10], Step: [1501/3125], Validation Acc: 62.2\n",
      "Epoch: [9/10], Step: [1601/3125], Validation Acc: 62.1\n",
      "Epoch: [9/10], Step: [1701/3125], Validation Acc: 62.6\n",
      "Epoch: [9/10], Step: [1801/3125], Validation Acc: 62.9\n",
      "Epoch: [9/10], Step: [1901/3125], Validation Acc: 62.5\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 63.1\n",
      "Epoch: [9/10], Step: [2101/3125], Validation Acc: 62.4\n",
      "Epoch: [9/10], Step: [2201/3125], Validation Acc: 63.4\n",
      "Epoch: [9/10], Step: [2301/3125], Validation Acc: 62.5\n",
      "Epoch: [9/10], Step: [2401/3125], Validation Acc: 63.1\n",
      "Epoch: [9/10], Step: [2501/3125], Validation Acc: 63.1\n",
      "Epoch: [9/10], Step: [2601/3125], Validation Acc: 63.2\n",
      "Epoch: [9/10], Step: [2701/3125], Validation Acc: 63.0\n",
      "Epoch: [9/10], Step: [2801/3125], Validation Acc: 62.8\n",
      "Epoch: [9/10], Step: [2901/3125], Validation Acc: 62.4\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 62.9\n",
      "Epoch: [9/10], Step: [3101/3125], Validation Acc: 62.7\n",
      "Epoch: [10/10], Step: [101/3125], Validation Acc: 63.4\n",
      "Epoch: [10/10], Step: [201/3125], Validation Acc: 62.7\n",
      "Epoch: [10/10], Step: [301/3125], Validation Acc: 63.6\n",
      "Epoch: [10/10], Step: [401/3125], Validation Acc: 64.6\n",
      "Epoch: [10/10], Step: [501/3125], Validation Acc: 62.8\n",
      "Epoch: [10/10], Step: [601/3125], Validation Acc: 62.9\n",
      "Epoch: [10/10], Step: [701/3125], Validation Acc: 61.9\n",
      "Epoch: [10/10], Step: [801/3125], Validation Acc: 62.5\n",
      "Epoch: [10/10], Step: [901/3125], Validation Acc: 62.2\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 62.3\n",
      "Epoch: [10/10], Step: [1101/3125], Validation Acc: 62.6\n",
      "Epoch: [10/10], Step: [1201/3125], Validation Acc: 63.0\n",
      "Epoch: [10/10], Step: [1301/3125], Validation Acc: 61.8\n",
      "Epoch: [10/10], Step: [1401/3125], Validation Acc: 61.7\n",
      "Epoch: [10/10], Step: [1501/3125], Validation Acc: 61.7\n",
      "Epoch: [10/10], Step: [1601/3125], Validation Acc: 61.7\n",
      "Epoch: [10/10], Step: [1701/3125], Validation Acc: 62.5\n",
      "Epoch: [10/10], Step: [1801/3125], Validation Acc: 61.8\n",
      "Epoch: [10/10], Step: [1901/3125], Validation Acc: 62.4\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 61.3\n",
      "Epoch: [10/10], Step: [2101/3125], Validation Acc: 62.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10/10], Step: [2201/3125], Validation Acc: 63.4\n",
      "Epoch: [10/10], Step: [2301/3125], Validation Acc: 62.0\n",
      "Epoch: [10/10], Step: [2401/3125], Validation Acc: 62.8\n",
      "Epoch: [10/10], Step: [2501/3125], Validation Acc: 62.7\n",
      "Epoch: [10/10], Step: [2601/3125], Validation Acc: 62.4\n",
      "Epoch: [10/10], Step: [2701/3125], Validation Acc: 64.1\n",
      "Epoch: [10/10], Step: [2801/3125], Validation Acc: 63.8\n",
      "Epoch: [10/10], Step: [2901/3125], Validation Acc: 64.0\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 64.0\n",
      "Epoch: [10/10], Step: [3101/3125], Validation Acc: 63.5\n"
     ]
    }
   ],
   "source": [
    "llist=[]\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data1, lengths1,data2, lengths2, labels in loader:\n",
    "        data_batch1, lengths_batch1,data_batch2, lengths_batch2, label_batch = data1, lengths1,data2, lengths2, labels\n",
    "        outputs = F.softmax(model(data_batch1, lengths_batch1,data_batch2, lengths_batch2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "        llist.append([data1,data2,labels,predicted])\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = CNN(emb_size=300, hidden_size=600, num_layers=2, num_classes=3, vocab_size=50002)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data1, lengths1, data2, lengths2, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data1, lengths1,data2, lengths2)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60.15806451612904,\n",
       " 63.62258064516128,\n",
       " 64.36451612903225,\n",
       " 64.33225806451614,\n",
       " 64.29354838709678,\n",
       " 63.490322580645156,\n",
       " 63.27096774193548,\n",
       " 63.00645161290323,\n",
       " 62.54193548387097,\n",
       " 62.73870967741936]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8360626601403759,\n",
       " 0.7915611267089844,\n",
       " 0.7235932157885644,\n",
       " 0.6791041022346865,\n",
       " 0.5987347585539664,\n",
       " 0.5031779129658976,\n",
       " 0.4353878550952481,\n",
       " 0.3277960462916282,\n",
       " 0.32129123662748643,\n",
       " 0.24950919732932123]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.36451612903225"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(epa)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
