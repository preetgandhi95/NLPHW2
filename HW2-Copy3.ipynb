{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets improve libraries that we are going to be used in this lab session\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(134)\n",
    "import pandas as pd\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('hw2_data/snli_train.tsv',delimiter='\\t',encoding='utf-8')\n",
    "val=pd.read_csv('hw2_data/snli_val.tsv',delimiter='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets=list(train['label'])\n",
    "val_targets=list(val['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_targets= [0 if x=='entailment' else x for x in val_targets]\n",
    "train_targets= [0 if x=='entailment' else x for x in train_targets]\n",
    "val_targets= [1 if x=='neutral' else x for x in val_targets]\n",
    "train_targets= [1 if x=='neutral' else x for x in train_targets]\n",
    "val_targets= [2 if x=='contradiction' else x for x in val_targets]\n",
    "train_targets= [2 if x=='contradiction' else x for x in train_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sen1= list(train['sentence1'])\n",
    "train_sen2= list(train['sentence2'])\n",
    "val_sen1= list(val['sentence1'])\n",
    "val_sen2= list(val['sentence2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "b=[]\n",
    "for i in range(0,100000):\n",
    "    tmp1=''\n",
    "    tmp2=''\n",
    "    for j in train_sen1[i].split(' '):\n",
    "        tmp1 =tmp1 + ' ' + j\n",
    "    for j in train_sen2[i].split(' '):\n",
    "        tmp2=tmp2 + ' ' + j\n",
    "    a.append(tmp1[1:])\n",
    "    b.append(tmp2[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sen1= a\n",
    "train_sen2= b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "b=[]\n",
    "for i in range(0,1000):\n",
    "    tmp1=''\n",
    "    tmp2=''\n",
    "    for j in val_sen1[i].split(' '):\n",
    "        tmp1 =tmp1 + ' ' + j\n",
    "    for j in val_sen2[i].split(' '):\n",
    "        tmp2=tmp2 + ' ' + j\n",
    "    a.append(tmp1[1:])\n",
    "    b.append(tmp2[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sen1= a\n",
    "val_sen2= b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write the tokenization function \n",
    "# set a hyperparameter - vocab size of dataset\n",
    "#Takes most frequent 10000 words from training set and makes it a vocabulary\n",
    "#Other words are put as unknown token.\n",
    "#One for unknown and one for padding and 9998 for most frequent words\n",
    "#Spacy will be used for tokenisation\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize(sent):\n",
    "  tokens = tokenizer(sent)\n",
    "  return [token.text for token in tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        \n",
    "        token_dataset.append(tokens)\n",
    "        \n",
    "    return token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_sen1=tokenize_dataset(train_sen1)\n",
    "tt_sen2=tokenize_dataset(train_sen2)\n",
    "tv_sen1=tokenize_dataset(val_sen1)\n",
    "tv_sen2=tokenize_dataset(val_sen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(tt_sen1, open(\"tt_sen1.p\", \"wb\"))\n",
    "pkl.dump(tt_sen2, open(\"tt_sen2.p\", \"wb\"))\n",
    "pkl.dump(tv_sen1, open(\"tv_sen1.p\", \"wb\"))\n",
    "pkl.dump(tv_sen2, open(\"tv_sen2.p\", \"wb\"))\n",
    "pkl.dump(train_targets, open(\"train_targets.p\", \"wb\"))\n",
    "pkl.dump(val_targets, open(\"val_targets.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "tt_sen1 = pkl.load(open(\"tt_sen1.p\", \"rb\"))\n",
    "tt_sen2 = pkl.load(open(\"tt_sen2.p\", \"rb\"))\n",
    "tv_sen1 = pkl.load(open(\"tv_sen1.p\", \"rb\"))\n",
    "tv_sen2 = pkl.load(open(\"tv_sen2.p\", \"rb\"))\n",
    "train_targets = pkl.load(open(\"train_targets.p\", \"rb\"))\n",
    "val_targets = pkl.load(open(\"val_targets.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_load = 50000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open('wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i\n",
    "        idx2words_ft[i] = s[0]\n",
    "        ordered_words_ft.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id2token\n",
    "x=idx2words_ft\n",
    "#token2idx\n",
    "y=words_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2words_ft={}\n",
    "idx2words_ft[0]='<pad>'\n",
    "idx2words_ft[1]='<unk>'\n",
    "for i in range(0,50000):\n",
    "    idx2words_ft[i+2]=x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=list(words_ft.keys())\n",
    "words_ft={}\n",
    "words_ft['<pad>']=0\n",
    "words_ft['<unk>']=1\n",
    "for i in range(0,50000):\n",
    "    words_ft[w[i]]=i+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=loaded_embeddings_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings_ft=np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings_ft=np.vstack([loaded_embeddings_ft,np.ones(300)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings_ft=np.vstack([loaded_embeddings_ft,z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id=words_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token=list(idx2words_ft.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 100000\n",
      "Train dataset size is 100000\n",
      "Val dataset size is 1000\n",
      "Val dataset size is 1000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices1 = token2index_dataset(tt_sen1)\n",
    "train_data_indices2 = token2index_dataset(tt_sen2)\n",
    "val_data_indices1 = token2index_dataset(tv_sen1)\n",
    "val_data_indices2 = token2index_dataset(tv_sen2)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices1)))\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices2)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices1)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm=0\n",
    "for i in range(0,100000):\n",
    "    r=max(len(train_data_indices1[i]),len(train_data_indices2[i]))\n",
    "    if mm<r:\n",
    "        mm=r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"    \n",
    "    def __init__(self, data_list1,data_list2, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1 = data_list1\n",
    "        self.data_list2 = data_list2\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list1) == len(self.target_list))\n",
    "        assert (len(self.data_list2) == len(self.target_list))\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "    def __len__(self):\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "        return len(self.data_list1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        token_idx1 = self.data_list1[key][:MAX_SENTENCE_LENGTH]       \n",
    "        token_idx2 = self.data_list2[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx1, len(token_idx1),token_idx2, len(token_idx2), label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = mm\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"    \n",
    "    def __init__(self, data_list1,data_list2, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1 = data_list1\n",
    "        self.data_list2 = data_list2\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list1) == len(self.target_list))\n",
    "        assert (len(self.data_list2) == len(self.target_list))\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "    def __len__(self):\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "        return len(self.data_list1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        token_idx1 = self.data_list1[key][:MAX_SENTENCE_LENGTH]       \n",
    "        token_idx2 = self.data_list2[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx1, len(token_idx1),token_idx2, len(token_idx2), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    length_list1 = []\n",
    "    data_list2 = []\n",
    "    length_list2 = []\n",
    "    label_list = []\n",
    "\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list1.append(datum[1])\n",
    "        length_list2.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list1.append(padded_vec1)\n",
    "        padded_vec2 = np.pad(np.array(datum[2]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list2.append(padded_vec2)\n",
    "        \n",
    "#     ind_dec_order1 = np.argsort(length_list1)[::-1]\n",
    "#     ind_dec_order2 = np.argsort(length_list2)[::-1]\n",
    "    \n",
    "#     data_list1 = np.array(data_list1)[ind_dec_order1]\n",
    "#     data_list2 = np.array(data_list2)[ind_dec_order2]\n",
    "    \n",
    "#     length_list1 = np.array(length_list1)[ind_dec_order1]\n",
    "#     length_list2 = np.array(length_list2)[ind_dec_order2]\n",
    "    \n",
    "#     label_list = np.array(label_list)[ind_dec_order]    \n",
    "        \n",
    "    return [torch.from_numpy(np.array(data_list1)), torch.from_numpy(np.array(np.int32(length_list1))) ,torch.from_numpy(np.array(data_list2)), torch.from_numpy(np.array(np.int32(length_list2))), torch.from_numpy(np.array(np.int32(label_list))).long()]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices1, train_data_indices2, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices1, val_data_indices2, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets implement basic Convolutional Neural Net model for text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.FloatTensor(loaded_embeddings_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight,freeze=True)\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "\n",
    "        self.linear = nn.Linear(2*hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x1, lengths1,x2, lengths2):\n",
    "        batch_size1, seq_len1 = x1.size()\n",
    "        batch_size2, seq_len2 = x2.size()\n",
    "        \n",
    "        embed1 = self.embedding(x1)\n",
    "        embed2 = self.embedding(x2)        \n",
    "        #First sentence\n",
    "        hidden1 = self.conv1(embed1.transpose(1,2)).transpose(1,2)\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size1, seq_len1, hidden1.size(-1))\n",
    "\n",
    "        hidden1 = self.conv2(hidden1.transpose(1,2)).transpose(1,2)\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size1, seq_len1, hidden1.size(-1))\n",
    "        hidden1 = torch.sum(hidden1, dim=1)\n",
    "       \n",
    "        \n",
    "        #second sentence\n",
    "        hidden2 = self.conv1(embed2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size2, seq_len2, hidden2.size(-1))\n",
    "\n",
    "        hidden2 = self.conv2(hidden2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size2, seq_len2, hidden2.size(-1))\n",
    "        hidden2 = torch.sum(hidden2, dim=1)\n",
    "        \n",
    "        hiddenfinal = torch.cat((hidden1, hidden2), 1)\n",
    "        logits = self.linear(hiddenfinal)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 42.4\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 53.6\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 53.7\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 52.2\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 55.9\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 56.8\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 57.6\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 58.8\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 58.2\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 59.3\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 59.4\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 59.6\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 60.7\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 60.4\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 61.1\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 60.5\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 62.3\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 60.6\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 63.4\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 61.0\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 61.7\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 62.5\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 62.0\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 62.0\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 64.5\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 62.5\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 61.5\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 62.2\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 61.9\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 62.3\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 61.7\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 62.9\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 61.6\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 64.5\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 62.6\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 62.3\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 63.6\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 64.3\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 62.6\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 62.5\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 62.8\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 63.3\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 61.7\n",
      "Epoch: [2/10], Step: [1301/3125], Validation Acc: 63.3\n",
      "Epoch: [2/10], Step: [1401/3125], Validation Acc: 63.4\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 62.4\n",
      "Epoch: [2/10], Step: [1601/3125], Validation Acc: 62.4\n",
      "Epoch: [2/10], Step: [1701/3125], Validation Acc: 63.9\n",
      "Epoch: [2/10], Step: [1801/3125], Validation Acc: 63.0\n",
      "Epoch: [2/10], Step: [1901/3125], Validation Acc: 64.8\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 64.3\n",
      "Epoch: [2/10], Step: [2101/3125], Validation Acc: 63.7\n",
      "Epoch: [2/10], Step: [2201/3125], Validation Acc: 63.9\n",
      "Epoch: [2/10], Step: [2301/3125], Validation Acc: 63.2\n",
      "Epoch: [2/10], Step: [2401/3125], Validation Acc: 64.2\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 64.1\n",
      "Epoch: [2/10], Step: [2601/3125], Validation Acc: 63.4\n",
      "Epoch: [2/10], Step: [2701/3125], Validation Acc: 63.0\n",
      "Epoch: [2/10], Step: [2801/3125], Validation Acc: 64.7\n",
      "Epoch: [2/10], Step: [2901/3125], Validation Acc: 64.4\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 64.6\n",
      "Epoch: [2/10], Step: [3101/3125], Validation Acc: 63.5\n",
      "Epoch: [3/10], Step: [101/3125], Validation Acc: 63.4\n",
      "Epoch: [3/10], Step: [201/3125], Validation Acc: 62.9\n",
      "Epoch: [3/10], Step: [301/3125], Validation Acc: 63.4\n",
      "Epoch: [3/10], Step: [401/3125], Validation Acc: 62.8\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 64.0\n",
      "Epoch: [3/10], Step: [601/3125], Validation Acc: 64.5\n",
      "Epoch: [3/10], Step: [701/3125], Validation Acc: 64.3\n",
      "Epoch: [3/10], Step: [801/3125], Validation Acc: 64.3\n",
      "Epoch: [3/10], Step: [901/3125], Validation Acc: 64.0\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 64.7\n",
      "Epoch: [3/10], Step: [1101/3125], Validation Acc: 62.5\n",
      "Epoch: [3/10], Step: [1201/3125], Validation Acc: 63.2\n",
      "Epoch: [3/10], Step: [1301/3125], Validation Acc: 64.4\n",
      "Epoch: [3/10], Step: [1401/3125], Validation Acc: 63.5\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 64.3\n",
      "Epoch: [3/10], Step: [1601/3125], Validation Acc: 62.4\n",
      "Epoch: [3/10], Step: [1701/3125], Validation Acc: 62.9\n",
      "Epoch: [3/10], Step: [1801/3125], Validation Acc: 63.2\n",
      "Epoch: [3/10], Step: [1901/3125], Validation Acc: 64.2\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 64.1\n",
      "Epoch: [3/10], Step: [2101/3125], Validation Acc: 64.5\n",
      "Epoch: [3/10], Step: [2201/3125], Validation Acc: 64.3\n",
      "Epoch: [3/10], Step: [2301/3125], Validation Acc: 64.3\n",
      "Epoch: [3/10], Step: [2401/3125], Validation Acc: 63.8\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 63.9\n",
      "Epoch: [3/10], Step: [2601/3125], Validation Acc: 64.1\n",
      "Epoch: [3/10], Step: [2701/3125], Validation Acc: 63.2\n",
      "Epoch: [3/10], Step: [2801/3125], Validation Acc: 64.5\n",
      "Epoch: [3/10], Step: [2901/3125], Validation Acc: 63.0\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 63.9\n",
      "Epoch: [3/10], Step: [3101/3125], Validation Acc: 63.0\n",
      "Epoch: [4/10], Step: [101/3125], Validation Acc: 63.9\n",
      "Epoch: [4/10], Step: [201/3125], Validation Acc: 64.9\n",
      "Epoch: [4/10], Step: [301/3125], Validation Acc: 65.7\n",
      "Epoch: [4/10], Step: [401/3125], Validation Acc: 64.0\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 64.8\n",
      "Epoch: [4/10], Step: [601/3125], Validation Acc: 64.9\n",
      "Epoch: [4/10], Step: [701/3125], Validation Acc: 64.8\n",
      "Epoch: [4/10], Step: [801/3125], Validation Acc: 64.6\n",
      "Epoch: [4/10], Step: [901/3125], Validation Acc: 64.8\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 63.7\n",
      "Epoch: [4/10], Step: [1101/3125], Validation Acc: 62.8\n",
      "Epoch: [4/10], Step: [1201/3125], Validation Acc: 65.2\n",
      "Epoch: [4/10], Step: [1301/3125], Validation Acc: 64.3\n",
      "Epoch: [4/10], Step: [1401/3125], Validation Acc: 62.7\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 64.1\n",
      "Epoch: [4/10], Step: [1601/3125], Validation Acc: 61.4\n",
      "Epoch: [4/10], Step: [1701/3125], Validation Acc: 64.6\n",
      "Epoch: [4/10], Step: [1801/3125], Validation Acc: 63.5\n",
      "Epoch: [4/10], Step: [1901/3125], Validation Acc: 62.8\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 63.6\n",
      "Epoch: [4/10], Step: [2101/3125], Validation Acc: 64.4\n",
      "Epoch: [4/10], Step: [2201/3125], Validation Acc: 63.8\n",
      "Epoch: [4/10], Step: [2301/3125], Validation Acc: 64.1\n",
      "Epoch: [4/10], Step: [2401/3125], Validation Acc: 62.4\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 63.5\n",
      "Epoch: [4/10], Step: [2601/3125], Validation Acc: 64.8\n",
      "Epoch: [4/10], Step: [2701/3125], Validation Acc: 64.2\n",
      "Epoch: [4/10], Step: [2801/3125], Validation Acc: 64.6\n",
      "Epoch: [4/10], Step: [2901/3125], Validation Acc: 63.6\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 65.1\n",
      "Epoch: [4/10], Step: [3101/3125], Validation Acc: 63.4\n",
      "Epoch: [5/10], Step: [101/3125], Validation Acc: 63.7\n",
      "Epoch: [5/10], Step: [201/3125], Validation Acc: 65.3\n",
      "Epoch: [5/10], Step: [301/3125], Validation Acc: 63.2\n",
      "Epoch: [5/10], Step: [401/3125], Validation Acc: 65.3\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 65.5\n",
      "Epoch: [5/10], Step: [601/3125], Validation Acc: 64.3\n",
      "Epoch: [5/10], Step: [701/3125], Validation Acc: 62.1\n",
      "Epoch: [5/10], Step: [801/3125], Validation Acc: 63.4\n",
      "Epoch: [5/10], Step: [901/3125], Validation Acc: 62.8\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 62.1\n",
      "Epoch: [5/10], Step: [1101/3125], Validation Acc: 63.6\n",
      "Epoch: [5/10], Step: [1201/3125], Validation Acc: 63.0\n",
      "Epoch: [5/10], Step: [1301/3125], Validation Acc: 63.4\n",
      "Epoch: [5/10], Step: [1401/3125], Validation Acc: 63.3\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 64.0\n",
      "Epoch: [5/10], Step: [1601/3125], Validation Acc: 64.1\n",
      "Epoch: [5/10], Step: [1701/3125], Validation Acc: 62.4\n",
      "Epoch: [5/10], Step: [1801/3125], Validation Acc: 64.1\n",
      "Epoch: [5/10], Step: [1901/3125], Validation Acc: 64.6\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 65.1\n",
      "Epoch: [5/10], Step: [2101/3125], Validation Acc: 63.5\n",
      "Epoch: [5/10], Step: [2201/3125], Validation Acc: 63.8\n",
      "Epoch: [5/10], Step: [2301/3125], Validation Acc: 63.9\n",
      "Epoch: [5/10], Step: [2401/3125], Validation Acc: 64.3\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 64.9\n",
      "Epoch: [5/10], Step: [2601/3125], Validation Acc: 64.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/10], Step: [2701/3125], Validation Acc: 64.8\n",
      "Epoch: [5/10], Step: [2801/3125], Validation Acc: 65.3\n",
      "Epoch: [5/10], Step: [2901/3125], Validation Acc: 65.6\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 64.3\n",
      "Epoch: [5/10], Step: [3101/3125], Validation Acc: 63.3\n",
      "Epoch: [6/10], Step: [101/3125], Validation Acc: 63.1\n",
      "Epoch: [6/10], Step: [201/3125], Validation Acc: 62.8\n",
      "Epoch: [6/10], Step: [301/3125], Validation Acc: 62.7\n",
      "Epoch: [6/10], Step: [401/3125], Validation Acc: 61.7\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 62.9\n",
      "Epoch: [6/10], Step: [601/3125], Validation Acc: 62.8\n",
      "Epoch: [6/10], Step: [701/3125], Validation Acc: 64.0\n",
      "Epoch: [6/10], Step: [801/3125], Validation Acc: 62.8\n",
      "Epoch: [6/10], Step: [901/3125], Validation Acc: 64.4\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 64.0\n",
      "Epoch: [6/10], Step: [1101/3125], Validation Acc: 62.6\n",
      "Epoch: [6/10], Step: [1201/3125], Validation Acc: 64.9\n",
      "Epoch: [6/10], Step: [1301/3125], Validation Acc: 63.8\n",
      "Epoch: [6/10], Step: [1401/3125], Validation Acc: 63.3\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 64.9\n",
      "Epoch: [6/10], Step: [1601/3125], Validation Acc: 62.2\n",
      "Epoch: [6/10], Step: [1701/3125], Validation Acc: 64.4\n",
      "Epoch: [6/10], Step: [1801/3125], Validation Acc: 63.1\n",
      "Epoch: [6/10], Step: [1901/3125], Validation Acc: 62.3\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 63.4\n",
      "Epoch: [6/10], Step: [2101/3125], Validation Acc: 64.4\n",
      "Epoch: [6/10], Step: [2201/3125], Validation Acc: 62.2\n",
      "Epoch: [6/10], Step: [2301/3125], Validation Acc: 63.1\n",
      "Epoch: [6/10], Step: [2401/3125], Validation Acc: 62.3\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 62.8\n",
      "Epoch: [6/10], Step: [2601/3125], Validation Acc: 63.2\n",
      "Epoch: [6/10], Step: [2701/3125], Validation Acc: 62.3\n",
      "Epoch: [6/10], Step: [2801/3125], Validation Acc: 61.4\n",
      "Epoch: [6/10], Step: [2901/3125], Validation Acc: 61.5\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 62.7\n",
      "Epoch: [6/10], Step: [3101/3125], Validation Acc: 63.3\n",
      "Epoch: [7/10], Step: [101/3125], Validation Acc: 64.4\n",
      "Epoch: [7/10], Step: [201/3125], Validation Acc: 63.3\n",
      "Epoch: [7/10], Step: [301/3125], Validation Acc: 64.4\n",
      "Epoch: [7/10], Step: [401/3125], Validation Acc: 62.9\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 63.8\n",
      "Epoch: [7/10], Step: [601/3125], Validation Acc: 63.1\n",
      "Epoch: [7/10], Step: [701/3125], Validation Acc: 63.3\n",
      "Epoch: [7/10], Step: [801/3125], Validation Acc: 63.7\n",
      "Epoch: [7/10], Step: [901/3125], Validation Acc: 63.5\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 63.8\n",
      "Epoch: [7/10], Step: [1101/3125], Validation Acc: 63.5\n",
      "Epoch: [7/10], Step: [1201/3125], Validation Acc: 62.6\n",
      "Epoch: [7/10], Step: [1301/3125], Validation Acc: 63.1\n",
      "Epoch: [7/10], Step: [1401/3125], Validation Acc: 62.2\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 62.9\n",
      "Epoch: [7/10], Step: [1601/3125], Validation Acc: 62.7\n",
      "Epoch: [7/10], Step: [1701/3125], Validation Acc: 61.5\n",
      "Epoch: [7/10], Step: [1801/3125], Validation Acc: 61.2\n",
      "Epoch: [7/10], Step: [1901/3125], Validation Acc: 63.5\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 63.5\n",
      "Epoch: [7/10], Step: [2101/3125], Validation Acc: 63.7\n",
      "Epoch: [7/10], Step: [2201/3125], Validation Acc: 62.8\n",
      "Epoch: [7/10], Step: [2301/3125], Validation Acc: 62.2\n",
      "Epoch: [7/10], Step: [2401/3125], Validation Acc: 62.9\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 62.6\n",
      "Epoch: [7/10], Step: [2601/3125], Validation Acc: 63.1\n",
      "Epoch: [7/10], Step: [2701/3125], Validation Acc: 63.4\n",
      "Epoch: [7/10], Step: [2801/3125], Validation Acc: 63.3\n",
      "Epoch: [7/10], Step: [2901/3125], Validation Acc: 63.2\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 61.2\n",
      "Epoch: [7/10], Step: [3101/3125], Validation Acc: 61.7\n",
      "Epoch: [8/10], Step: [101/3125], Validation Acc: 62.1\n",
      "Epoch: [8/10], Step: [201/3125], Validation Acc: 62.8\n",
      "Epoch: [8/10], Step: [301/3125], Validation Acc: 63.5\n",
      "Epoch: [8/10], Step: [401/3125], Validation Acc: 64.1\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 62.7\n",
      "Epoch: [8/10], Step: [601/3125], Validation Acc: 62.7\n",
      "Epoch: [8/10], Step: [701/3125], Validation Acc: 62.2\n",
      "Epoch: [8/10], Step: [801/3125], Validation Acc: 62.8\n",
      "Epoch: [8/10], Step: [901/3125], Validation Acc: 61.1\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 63.0\n",
      "Epoch: [8/10], Step: [1101/3125], Validation Acc: 64.8\n",
      "Epoch: [8/10], Step: [1201/3125], Validation Acc: 61.8\n",
      "Epoch: [8/10], Step: [1301/3125], Validation Acc: 62.2\n",
      "Epoch: [8/10], Step: [1401/3125], Validation Acc: 64.0\n",
      "Epoch: [8/10], Step: [1501/3125], Validation Acc: 62.2\n",
      "Epoch: [8/10], Step: [1601/3125], Validation Acc: 60.6\n",
      "Epoch: [8/10], Step: [1701/3125], Validation Acc: 63.5\n",
      "Epoch: [8/10], Step: [1801/3125], Validation Acc: 60.8\n",
      "Epoch: [8/10], Step: [1901/3125], Validation Acc: 62.1\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 60.8\n",
      "Epoch: [8/10], Step: [2101/3125], Validation Acc: 62.4\n",
      "Epoch: [8/10], Step: [2201/3125], Validation Acc: 62.5\n",
      "Epoch: [8/10], Step: [2301/3125], Validation Acc: 62.3\n",
      "Epoch: [8/10], Step: [2401/3125], Validation Acc: 62.7\n",
      "Epoch: [8/10], Step: [2501/3125], Validation Acc: 63.0\n",
      "Epoch: [8/10], Step: [2601/3125], Validation Acc: 62.6\n",
      "Epoch: [8/10], Step: [2701/3125], Validation Acc: 63.8\n",
      "Epoch: [8/10], Step: [2801/3125], Validation Acc: 62.2\n",
      "Epoch: [8/10], Step: [2901/3125], Validation Acc: 63.0\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 63.1\n",
      "Epoch: [8/10], Step: [3101/3125], Validation Acc: 61.9\n",
      "Epoch: [9/10], Step: [101/3125], Validation Acc: 62.2\n",
      "Epoch: [9/10], Step: [201/3125], Validation Acc: 61.7\n",
      "Epoch: [9/10], Step: [301/3125], Validation Acc: 61.6\n",
      "Epoch: [9/10], Step: [401/3125], Validation Acc: 62.2\n",
      "Epoch: [9/10], Step: [501/3125], Validation Acc: 62.3\n",
      "Epoch: [9/10], Step: [601/3125], Validation Acc: 61.1\n",
      "Epoch: [9/10], Step: [701/3125], Validation Acc: 60.8\n",
      "Epoch: [9/10], Step: [801/3125], Validation Acc: 63.9\n",
      "Epoch: [9/10], Step: [901/3125], Validation Acc: 62.4\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 62.8\n",
      "Epoch: [9/10], Step: [1101/3125], Validation Acc: 64.1\n",
      "Epoch: [9/10], Step: [1201/3125], Validation Acc: 61.8\n",
      "Epoch: [9/10], Step: [1301/3125], Validation Acc: 62.3\n",
      "Epoch: [9/10], Step: [1401/3125], Validation Acc: 63.5\n",
      "Epoch: [9/10], Step: [1501/3125], Validation Acc: 63.1\n",
      "Epoch: [9/10], Step: [1601/3125], Validation Acc: 61.9\n",
      "Epoch: [9/10], Step: [1701/3125], Validation Acc: 62.4\n",
      "Epoch: [9/10], Step: [1801/3125], Validation Acc: 62.9\n",
      "Epoch: [9/10], Step: [1901/3125], Validation Acc: 62.0\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 63.1\n",
      "Epoch: [9/10], Step: [2101/3125], Validation Acc: 62.8\n",
      "Epoch: [9/10], Step: [2201/3125], Validation Acc: 61.4\n",
      "Epoch: [9/10], Step: [2301/3125], Validation Acc: 63.2\n",
      "Epoch: [9/10], Step: [2401/3125], Validation Acc: 61.7\n",
      "Epoch: [9/10], Step: [2501/3125], Validation Acc: 63.0\n",
      "Epoch: [9/10], Step: [2601/3125], Validation Acc: 62.0\n",
      "Epoch: [9/10], Step: [2701/3125], Validation Acc: 64.9\n",
      "Epoch: [9/10], Step: [2801/3125], Validation Acc: 62.7\n",
      "Epoch: [9/10], Step: [2901/3125], Validation Acc: 62.6\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 61.7\n",
      "Epoch: [9/10], Step: [3101/3125], Validation Acc: 63.1\n",
      "Epoch: [10/10], Step: [101/3125], Validation Acc: 62.8\n",
      "Epoch: [10/10], Step: [201/3125], Validation Acc: 62.8\n",
      "Epoch: [10/10], Step: [301/3125], Validation Acc: 61.6\n",
      "Epoch: [10/10], Step: [401/3125], Validation Acc: 62.0\n",
      "Epoch: [10/10], Step: [501/3125], Validation Acc: 62.6\n",
      "Epoch: [10/10], Step: [601/3125], Validation Acc: 61.3\n",
      "Epoch: [10/10], Step: [701/3125], Validation Acc: 62.6\n",
      "Epoch: [10/10], Step: [801/3125], Validation Acc: 63.1\n",
      "Epoch: [10/10], Step: [901/3125], Validation Acc: 63.0\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 62.6\n",
      "Epoch: [10/10], Step: [1101/3125], Validation Acc: 63.6\n",
      "Epoch: [10/10], Step: [1201/3125], Validation Acc: 62.9\n",
      "Epoch: [10/10], Step: [1301/3125], Validation Acc: 61.2\n",
      "Epoch: [10/10], Step: [1401/3125], Validation Acc: 62.0\n",
      "Epoch: [10/10], Step: [1501/3125], Validation Acc: 62.1\n",
      "Epoch: [10/10], Step: [1601/3125], Validation Acc: 61.9\n",
      "Epoch: [10/10], Step: [1701/3125], Validation Acc: 63.8\n",
      "Epoch: [10/10], Step: [1801/3125], Validation Acc: 62.4\n",
      "Epoch: [10/10], Step: [1901/3125], Validation Acc: 61.5\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 62.4\n",
      "Epoch: [10/10], Step: [2101/3125], Validation Acc: 60.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10/10], Step: [2201/3125], Validation Acc: 63.4\n",
      "Epoch: [10/10], Step: [2301/3125], Validation Acc: 62.4\n",
      "Epoch: [10/10], Step: [2401/3125], Validation Acc: 62.1\n",
      "Epoch: [10/10], Step: [2501/3125], Validation Acc: 60.8\n",
      "Epoch: [10/10], Step: [2601/3125], Validation Acc: 62.4\n",
      "Epoch: [10/10], Step: [2701/3125], Validation Acc: 62.8\n",
      "Epoch: [10/10], Step: [2801/3125], Validation Acc: 62.1\n",
      "Epoch: [10/10], Step: [2901/3125], Validation Acc: 62.7\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 59.9\n",
      "Epoch: [10/10], Step: [3101/3125], Validation Acc: 61.3\n"
     ]
    }
   ],
   "source": [
    "llist=[]\n",
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data1, lengths1,data2, lengths2, labels in loader:\n",
    "        data_batch1, lengths_batch1,data_batch2, lengths_batch2, label_batch = data1, lengths1,data2, lengths2, labels\n",
    "        outputs = F.softmax(model(data_batch1, lengths_batch1,data_batch2, lengths_batch2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "        llist.append([data1,data2,labels,predicted])\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = CNN(emb_size=300, hidden_size=500, num_layers=2, num_classes=3, vocab_size=50002)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data1, lengths1, data2, lengths2, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data1, lengths1,data2, lengths2)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59.429032258064524,\n",
       " 63.38387096774194,\n",
       " 63.725806451612904,\n",
       " 64.03225806451613,\n",
       " 63.99032258064515,\n",
       " 63.07419354838709,\n",
       " 63.000000000000014,\n",
       " 62.55806451612902,\n",
       " 62.49032258064516,\n",
       " 62.209677419354854]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8947979961672137,\n",
       " 0.8048133888552266,\n",
       " 0.7067869488270052,\n",
       " 0.6557261117043034,\n",
       " 0.649998927308667,\n",
       " 0.5383199251467182,\n",
       " 0.4668583370024158,\n",
       " 0.38761636326389926,\n",
       " 0.34400838949987966,\n",
       " 0.30671721216171016]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.03225806451613"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(epa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight,freeze=True)\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(2*hidden_size, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x1, lengths1, x2, lengths2):\n",
    "        # reset hidden state\n",
    "\n",
    "        batch_size1, seq_len1 = x1.size()\n",
    "        batch_size2, seq_len2 = x2.size()\n",
    "        \n",
    "        self.hidden1 = self.init_hidden(batch_size1)\n",
    "        self.hidden2 = self.init_hidden(batch_size2)\n",
    "        # get embedding of characters\n",
    "        embed1 = self.embedding(x1)\n",
    "        embed2 = self.embedding(x2)\n",
    "        # pack padded sequence\n",
    "        #Pytorch wants sequences to be in descending order. First one have longest length and\n",
    "        #Then the subsequent ones have lesser lengths\n",
    "        embed1 = torch.nn.utils.rnn.pack_padded_sequence(embed1, lengths1.numpy(), batch_first=True)\n",
    "        embed2 = torch.nn.utils.rnn.pack_padded_sequence(embed2, lengths2.numpy(), batch_first=True)\n",
    "        \n",
    "        # fprop though RNN\n",
    "        rnn_out1, self.hidden1 = self.rnn(embed1, self.hidden1)\n",
    "        rnn_out2, self.hidden2 = self.rnn(embed2, self.hidden2)\n",
    "        \n",
    "        # undo packing\n",
    "        rnn_out1, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out1, batch_first=True)\n",
    "        rnn_out2, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out2, batch_first=True)\n",
    "        # sum hidden activations of RNN across time\n",
    "        rnn_out1 = torch.sum(rnn_out1, dim=1)\n",
    "        rnn_out2 = torch.sum(rnn_out2, dim=1)\n",
    "\n",
    "        \n",
    "        \n",
    "        rnnoutfinal = torch.cat((rnn_out1, rnn_out2), 1)\n",
    "\n",
    "        logits = self.linear(rnnoutfinal)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'lengths' array has to be sorted in decreasing order",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-ddc563ad13f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-179-e31627149d8c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, lengths1, x2, lengths2)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#Pytorch wants sequences to be in descending order. First one have longest length and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#Then the subsequent ones have lesser lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0membed1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0membed2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# fast pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmight_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_tensors_permissive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackPadded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/packing.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, lengths, batch_first)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mprev_l\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'lengths' array has to be sorted in decreasing order\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'lengths' array has to be sorted in decreasing order"
     ]
    }
   ],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data1, lengths1,data2, lengths2, labels in loader:\n",
    "        data_batch1, lengths_batch1,data_batch2, lengths_batch2, label_batch = data1, lengths1, labels\n",
    "        outputs = F.softmax(model(data_batch1, lengths_batch1,data_batch2, lengths_batch2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = RNN(emb_size=300, hidden_size=200, num_layers=2, num_classes=3, vocab_size=50002)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data1, lengths1,data2, lengths2, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data1, lengths1,data2, lengths2)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
