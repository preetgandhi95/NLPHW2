{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets improve libraries that we are going to be used in this lab session\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(134)\n",
    "import pandas as pd\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "tt_sen1 = pkl.load(open(\"tt_sen1.p\", \"rb\"))\n",
    "tt_sen2 = pkl.load(open(\"tt_sen2.p\", \"rb\"))\n",
    "tv_sen1 = pkl.load(open(\"tv_sen1.p\", \"rb\"))\n",
    "tv_sen2 = pkl.load(open(\"tv_sen2.p\", \"rb\"))\n",
    "train_targets = pkl.load(open(\"train_targets.p\", \"rb\"))\n",
    "val_targets = pkl.load(open(\"val_targets.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_load = 50000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open('wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i\n",
    "        idx2words_ft[i] = s[0]\n",
    "        ordered_words_ft.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id2token\n",
    "x=idx2words_ft\n",
    "#token2idx\n",
    "y=words_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2words_ft={}\n",
    "idx2words_ft[0]='<pad>'\n",
    "idx2words_ft[1]='<unk>'\n",
    "for i in range(0,50000):\n",
    "    idx2words_ft[i+2]=x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=list(words_ft.keys())\n",
    "words_ft={}\n",
    "words_ft['<pad>']=0\n",
    "words_ft['<unk>']=1\n",
    "for i in range(0,50000):\n",
    "    words_ft[w[i]]=i+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=loaded_embeddings_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings_ft=np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings_ft=np.vstack([loaded_embeddings_ft,np.ones(300)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings_ft=np.vstack([loaded_embeddings_ft,z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id=words_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token=list(idx2words_ft.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 100000\n",
      "Train dataset size is 100000\n",
      "Val dataset size is 1000\n",
      "Val dataset size is 1000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices1 = token2index_dataset(tt_sen1)\n",
    "train_data_indices2 = token2index_dataset(tt_sen2)\n",
    "val_data_indices1 = token2index_dataset(tv_sen1)\n",
    "val_data_indices2 = token2index_dataset(tv_sen2)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices1)))\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices2)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices1)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm=0\n",
    "for i in range(0,100000):\n",
    "    r=max(len(train_data_indices1[i]),len(train_data_indices2[i]))\n",
    "    if mm<r:\n",
    "        mm=r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"    \n",
    "    def __init__(self, data_list1,data_list2, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1 = data_list1\n",
    "        self.data_list2 = data_list2\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list1) == len(self.target_list))\n",
    "        assert (len(self.data_list2) == len(self.target_list))\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "    def __len__(self):\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "        return len(self.data_list1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        token_idx1 = self.data_list1[key][:MAX_SENTENCE_LENGTH]       \n",
    "        token_idx2 = self.data_list2[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx1, len(token_idx1),token_idx2, len(token_idx2), label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = mm\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"    \n",
    "    def __init__(self, data_list1,data_list2, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1 = data_list1\n",
    "        self.data_list2 = data_list2\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list1) == len(self.target_list))\n",
    "        assert (len(self.data_list2) == len(self.target_list))\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "    def __len__(self):\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "        return len(self.data_list1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        token_idx1 = self.data_list1[key][:MAX_SENTENCE_LENGTH]       \n",
    "        token_idx2 = self.data_list2[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx1, len(token_idx1),token_idx2, len(token_idx2), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    length_list1 = []\n",
    "    data_list2 = []\n",
    "    length_list2 = []\n",
    "    label_list = []\n",
    "\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list1.append(datum[1])\n",
    "        length_list2.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list1.append(padded_vec1)\n",
    "        padded_vec2 = np.pad(np.array(datum[2]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list2.append(padded_vec2)\n",
    "        \n",
    "    ind_dec_order1 = np.argsort(length_list1)[::-1]\n",
    "    ind_dec_order2 = np.argsort(length_list2)[::-1]\n",
    "    \n",
    "    data_list1 = np.array(data_list1)[ind_dec_order1]\n",
    "    data_list2 = np.array(data_list2)[ind_dec_order2]\n",
    "    \n",
    "    length_list1 = np.array(length_list1)[ind_dec_order1]\n",
    "    length_list2 = np.array(length_list2)[ind_dec_order2]\n",
    "    \n",
    "#     label_list = np.array(label_list)[ind_dec_order]    \n",
    "        \n",
    "    return [torch.from_numpy(np.array(data_list1)), torch.from_numpy(np.array(np.int32(length_list1))) ,torch.from_numpy(np.array(data_list2)), torch.from_numpy(np.array(np.int32(length_list2))), torch.from_numpy(np.array(np.int32(label_list))).long(),ind_dec_order1,ind_dec_order2]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices1, train_data_indices2, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices1, val_data_indices2, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets implement basic Convolutional Neural Net model for text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.FloatTensor(loaded_embeddings_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight,freeze=True)\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(2*hidden_size, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x1, lengths1, x2, lengths2,id1,id2):\n",
    "        # reset hidden state\n",
    "\n",
    "        batch_size1, seq_len1 = x1.size()\n",
    "        batch_size2, seq_len2 = x2.size()\n",
    "        \n",
    "        self.hidden1 = self.init_hidden(batch_size1)\n",
    "        self.hidden2 = self.init_hidden(batch_size2)\n",
    "        # get embedding of characters\n",
    "        embed1 = self.embedding(x1)\n",
    "        embed2 = self.embedding(x2)\n",
    "        # pack padded sequence\n",
    "        #Pytorch wants sequences to be in descending order. First one have longest length and\n",
    "        #Then the subsequent ones have lesser lengths\n",
    "        embed1 = torch.nn.utils.rnn.pack_padded_sequence(embed1, lengths1.numpy(), batch_first=True)\n",
    "        embed2 = torch.nn.utils.rnn.pack_padded_sequence(embed2, lengths2.numpy(), batch_first=True)\n",
    "        \n",
    "        # fprop though RNN\n",
    "        rnn_out1, self.hidden1 = self.rnn(embed1, self.hidden1)\n",
    "        rnn_out2, self.hidden2 = self.rnn(embed2, self.hidden2)\n",
    "        \n",
    "        # undo packing\n",
    "        rnn_out1, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out1, batch_first=True)\n",
    "        rnn_out2, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out2, batch_first=True)\n",
    "\n",
    "        # sum hidden activations of RNN across time\n",
    "        rnn_out1 = torch.sum(rnn_out1, dim=1)\n",
    "        rnn_out2 = torch.sum(rnn_out2, dim=1)\n",
    "        \n",
    "\n",
    "        r1=rnn_out1.clone()\n",
    "        r2=rnn_out2.clone()\n",
    "        for i in range(0,len(r1)):\n",
    "            rnn_out1[i]=r1[id1[i]]\n",
    "        for i in range(0,len(r2)):\n",
    "            rnn_out2[i]=r2[id2[i]]\n",
    "        rnnoutfinal = torch.cat((rnn_out1, rnn_out2), 1)\n",
    "\n",
    "        logits = self.linear(rnnoutfinal)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 54.7\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 55.2\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 52.6\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 51.8\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 53.9\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 52.8\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 55.4\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 54.3\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 55.2\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 52.2\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 53.4\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 54.6\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 52.9\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 52.6\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 54.8\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 53.9\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 51.8\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 51.5\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 51.4\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 51.7\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 53.2\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 54.8\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 52.8\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 54.8\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 53.3\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 52.5\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 53.8\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 54.1\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 52.8\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 50.5\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 53.7\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 55.4\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 51.9\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 53.3\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 53.8\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 53.1\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 54.7\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 53.1\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 54.3\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 53.1\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 56.4\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 54.4\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 54.3\n",
      "Epoch: [2/10], Step: [1301/3125], Validation Acc: 53.8\n",
      "Epoch: [2/10], Step: [1401/3125], Validation Acc: 51.5\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 55.1\n",
      "Epoch: [2/10], Step: [1601/3125], Validation Acc: 55.0\n",
      "Epoch: [2/10], Step: [1701/3125], Validation Acc: 50.6\n",
      "Epoch: [2/10], Step: [1801/3125], Validation Acc: 51.8\n",
      "Epoch: [2/10], Step: [1901/3125], Validation Acc: 55.1\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 52.3\n",
      "Epoch: [2/10], Step: [2101/3125], Validation Acc: 52.4\n",
      "Epoch: [2/10], Step: [2201/3125], Validation Acc: 53.9\n",
      "Epoch: [2/10], Step: [2301/3125], Validation Acc: 55.2\n",
      "Epoch: [2/10], Step: [2401/3125], Validation Acc: 51.9\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 52.3\n",
      "Epoch: [2/10], Step: [2601/3125], Validation Acc: 51.5\n",
      "Epoch: [2/10], Step: [2701/3125], Validation Acc: 53.2\n",
      "Epoch: [2/10], Step: [2801/3125], Validation Acc: 52.6\n",
      "Epoch: [2/10], Step: [2901/3125], Validation Acc: 55.2\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 55.0\n",
      "Epoch: [2/10], Step: [3101/3125], Validation Acc: 53.9\n",
      "Epoch: [3/10], Step: [101/3125], Validation Acc: 50.5\n",
      "Epoch: [3/10], Step: [201/3125], Validation Acc: 53.5\n",
      "Epoch: [3/10], Step: [301/3125], Validation Acc: 54.4\n",
      "Epoch: [3/10], Step: [401/3125], Validation Acc: 54.7\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 53.9\n",
      "Epoch: [3/10], Step: [601/3125], Validation Acc: 53.0\n",
      "Epoch: [3/10], Step: [701/3125], Validation Acc: 53.6\n",
      "Epoch: [3/10], Step: [801/3125], Validation Acc: 54.6\n",
      "Epoch: [3/10], Step: [901/3125], Validation Acc: 52.3\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 51.9\n",
      "Epoch: [3/10], Step: [1101/3125], Validation Acc: 55.0\n",
      "Epoch: [3/10], Step: [1201/3125], Validation Acc: 52.3\n",
      "Epoch: [3/10], Step: [1301/3125], Validation Acc: 50.5\n",
      "Epoch: [3/10], Step: [1401/3125], Validation Acc: 52.7\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 55.6\n",
      "Epoch: [3/10], Step: [1601/3125], Validation Acc: 51.9\n",
      "Epoch: [3/10], Step: [1701/3125], Validation Acc: 53.1\n",
      "Epoch: [3/10], Step: [1801/3125], Validation Acc: 53.8\n",
      "Epoch: [3/10], Step: [1901/3125], Validation Acc: 52.5\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 52.5\n",
      "Epoch: [3/10], Step: [2101/3125], Validation Acc: 54.6\n",
      "Epoch: [3/10], Step: [2201/3125], Validation Acc: 52.6\n",
      "Epoch: [3/10], Step: [2301/3125], Validation Acc: 53.2\n",
      "Epoch: [3/10], Step: [2401/3125], Validation Acc: 53.1\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 51.8\n",
      "Epoch: [3/10], Step: [2601/3125], Validation Acc: 52.0\n",
      "Epoch: [3/10], Step: [2701/3125], Validation Acc: 55.7\n",
      "Epoch: [3/10], Step: [2801/3125], Validation Acc: 52.3\n",
      "Epoch: [3/10], Step: [2901/3125], Validation Acc: 50.6\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 54.9\n",
      "Epoch: [3/10], Step: [3101/3125], Validation Acc: 53.8\n",
      "Epoch: [4/10], Step: [101/3125], Validation Acc: 54.5\n",
      "Epoch: [4/10], Step: [201/3125], Validation Acc: 55.8\n",
      "Epoch: [4/10], Step: [301/3125], Validation Acc: 50.9\n",
      "Epoch: [4/10], Step: [401/3125], Validation Acc: 52.9\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 53.0\n",
      "Epoch: [4/10], Step: [601/3125], Validation Acc: 54.0\n",
      "Epoch: [4/10], Step: [701/3125], Validation Acc: 52.5\n",
      "Epoch: [4/10], Step: [801/3125], Validation Acc: 52.3\n",
      "Epoch: [4/10], Step: [901/3125], Validation Acc: 56.5\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 53.8\n",
      "Epoch: [4/10], Step: [1101/3125], Validation Acc: 54.7\n",
      "Epoch: [4/10], Step: [1201/3125], Validation Acc: 50.5\n",
      "Epoch: [4/10], Step: [1301/3125], Validation Acc: 52.6\n",
      "Epoch: [4/10], Step: [1401/3125], Validation Acc: 56.0\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 53.5\n",
      "Epoch: [4/10], Step: [1601/3125], Validation Acc: 51.8\n",
      "Epoch: [4/10], Step: [1701/3125], Validation Acc: 54.8\n",
      "Epoch: [4/10], Step: [1801/3125], Validation Acc: 50.6\n",
      "Epoch: [4/10], Step: [1901/3125], Validation Acc: 52.4\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 54.7\n",
      "Epoch: [4/10], Step: [2101/3125], Validation Acc: 53.6\n",
      "Epoch: [4/10], Step: [2201/3125], Validation Acc: 53.2\n",
      "Epoch: [4/10], Step: [2301/3125], Validation Acc: 52.8\n",
      "Epoch: [4/10], Step: [2401/3125], Validation Acc: 56.1\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 51.8\n",
      "Epoch: [4/10], Step: [2601/3125], Validation Acc: 54.8\n",
      "Epoch: [4/10], Step: [2701/3125], Validation Acc: 54.2\n",
      "Epoch: [4/10], Step: [2801/3125], Validation Acc: 55.1\n",
      "Epoch: [4/10], Step: [2901/3125], Validation Acc: 52.8\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 53.0\n",
      "Epoch: [4/10], Step: [3101/3125], Validation Acc: 54.2\n",
      "Epoch: [5/10], Step: [101/3125], Validation Acc: 52.1\n",
      "Epoch: [5/10], Step: [201/3125], Validation Acc: 52.0\n",
      "Epoch: [5/10], Step: [301/3125], Validation Acc: 51.8\n",
      "Epoch: [5/10], Step: [401/3125], Validation Acc: 53.1\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 55.0\n",
      "Epoch: [5/10], Step: [601/3125], Validation Acc: 52.5\n",
      "Epoch: [5/10], Step: [701/3125], Validation Acc: 52.5\n",
      "Epoch: [5/10], Step: [801/3125], Validation Acc: 55.6\n",
      "Epoch: [5/10], Step: [901/3125], Validation Acc: 51.3\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 54.4\n",
      "Epoch: [5/10], Step: [1101/3125], Validation Acc: 54.6\n",
      "Epoch: [5/10], Step: [1201/3125], Validation Acc: 53.1\n",
      "Epoch: [5/10], Step: [1301/3125], Validation Acc: 52.6\n",
      "Epoch: [5/10], Step: [1401/3125], Validation Acc: 54.9\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 53.5\n",
      "Epoch: [5/10], Step: [1601/3125], Validation Acc: 55.2\n",
      "Epoch: [5/10], Step: [1701/3125], Validation Acc: 53.9\n",
      "Epoch: [5/10], Step: [1801/3125], Validation Acc: 53.0\n",
      "Epoch: [5/10], Step: [1901/3125], Validation Acc: 57.7\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 51.8\n",
      "Epoch: [5/10], Step: [2101/3125], Validation Acc: 52.7\n",
      "Epoch: [5/10], Step: [2201/3125], Validation Acc: 54.2\n",
      "Epoch: [5/10], Step: [2301/3125], Validation Acc: 54.4\n",
      "Epoch: [5/10], Step: [2401/3125], Validation Acc: 51.6\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 54.1\n",
      "Epoch: [5/10], Step: [2601/3125], Validation Acc: 52.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/10], Step: [2701/3125], Validation Acc: 53.4\n",
      "Epoch: [5/10], Step: [2801/3125], Validation Acc: 53.5\n",
      "Epoch: [5/10], Step: [2901/3125], Validation Acc: 54.0\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 57.3\n",
      "Epoch: [5/10], Step: [3101/3125], Validation Acc: 52.6\n",
      "Epoch: [6/10], Step: [101/3125], Validation Acc: 56.1\n",
      "Epoch: [6/10], Step: [201/3125], Validation Acc: 54.2\n",
      "Epoch: [6/10], Step: [301/3125], Validation Acc: 53.6\n",
      "Epoch: [6/10], Step: [401/3125], Validation Acc: 52.5\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 53.7\n",
      "Epoch: [6/10], Step: [601/3125], Validation Acc: 55.4\n",
      "Epoch: [6/10], Step: [701/3125], Validation Acc: 52.9\n",
      "Epoch: [6/10], Step: [801/3125], Validation Acc: 54.2\n",
      "Epoch: [6/10], Step: [901/3125], Validation Acc: 54.3\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 51.0\n",
      "Epoch: [6/10], Step: [1101/3125], Validation Acc: 57.0\n",
      "Epoch: [6/10], Step: [1201/3125], Validation Acc: 52.9\n",
      "Epoch: [6/10], Step: [1301/3125], Validation Acc: 52.3\n",
      "Epoch: [6/10], Step: [1401/3125], Validation Acc: 52.6\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 49.9\n",
      "Epoch: [6/10], Step: [1601/3125], Validation Acc: 53.7\n",
      "Epoch: [6/10], Step: [1701/3125], Validation Acc: 52.0\n",
      "Epoch: [6/10], Step: [1801/3125], Validation Acc: 55.2\n",
      "Epoch: [6/10], Step: [1901/3125], Validation Acc: 52.4\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 55.4\n",
      "Epoch: [6/10], Step: [2101/3125], Validation Acc: 53.3\n",
      "Epoch: [6/10], Step: [2201/3125], Validation Acc: 53.7\n",
      "Epoch: [6/10], Step: [2301/3125], Validation Acc: 53.9\n",
      "Epoch: [6/10], Step: [2401/3125], Validation Acc: 54.4\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 51.7\n",
      "Epoch: [6/10], Step: [2601/3125], Validation Acc: 53.6\n",
      "Epoch: [6/10], Step: [2701/3125], Validation Acc: 54.4\n",
      "Epoch: [6/10], Step: [2801/3125], Validation Acc: 53.6\n",
      "Epoch: [6/10], Step: [2901/3125], Validation Acc: 53.0\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 55.2\n",
      "Epoch: [6/10], Step: [3101/3125], Validation Acc: 52.6\n",
      "Epoch: [7/10], Step: [101/3125], Validation Acc: 52.3\n",
      "Epoch: [7/10], Step: [201/3125], Validation Acc: 56.1\n",
      "Epoch: [7/10], Step: [301/3125], Validation Acc: 54.2\n",
      "Epoch: [7/10], Step: [401/3125], Validation Acc: 51.4\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 55.9\n",
      "Epoch: [7/10], Step: [601/3125], Validation Acc: 50.5\n",
      "Epoch: [7/10], Step: [701/3125], Validation Acc: 55.8\n",
      "Epoch: [7/10], Step: [801/3125], Validation Acc: 54.5\n",
      "Epoch: [7/10], Step: [901/3125], Validation Acc: 51.5\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 55.2\n",
      "Epoch: [7/10], Step: [1101/3125], Validation Acc: 52.9\n",
      "Epoch: [7/10], Step: [1201/3125], Validation Acc: 55.7\n",
      "Epoch: [7/10], Step: [1301/3125], Validation Acc: 53.0\n",
      "Epoch: [7/10], Step: [1401/3125], Validation Acc: 53.6\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 52.0\n",
      "Epoch: [7/10], Step: [1601/3125], Validation Acc: 53.8\n",
      "Epoch: [7/10], Step: [1701/3125], Validation Acc: 52.0\n",
      "Epoch: [7/10], Step: [1801/3125], Validation Acc: 52.2\n",
      "Epoch: [7/10], Step: [1901/3125], Validation Acc: 55.3\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 52.9\n",
      "Epoch: [7/10], Step: [2101/3125], Validation Acc: 53.0\n",
      "Epoch: [7/10], Step: [2201/3125], Validation Acc: 56.8\n",
      "Epoch: [7/10], Step: [2301/3125], Validation Acc: 53.5\n",
      "Epoch: [7/10], Step: [2401/3125], Validation Acc: 55.7\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 53.4\n",
      "Epoch: [7/10], Step: [2601/3125], Validation Acc: 51.9\n",
      "Epoch: [7/10], Step: [2701/3125], Validation Acc: 55.0\n",
      "Epoch: [7/10], Step: [2801/3125], Validation Acc: 51.8\n",
      "Epoch: [7/10], Step: [2901/3125], Validation Acc: 52.8\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 51.4\n",
      "Epoch: [7/10], Step: [3101/3125], Validation Acc: 53.6\n",
      "Epoch: [8/10], Step: [101/3125], Validation Acc: 55.4\n",
      "Epoch: [8/10], Step: [201/3125], Validation Acc: 52.5\n",
      "Epoch: [8/10], Step: [301/3125], Validation Acc: 52.9\n",
      "Epoch: [8/10], Step: [401/3125], Validation Acc: 54.7\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 51.8\n",
      "Epoch: [8/10], Step: [601/3125], Validation Acc: 54.3\n",
      "Epoch: [8/10], Step: [701/3125], Validation Acc: 52.2\n",
      "Epoch: [8/10], Step: [801/3125], Validation Acc: 52.2\n",
      "Epoch: [8/10], Step: [901/3125], Validation Acc: 52.3\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 55.5\n",
      "Epoch: [8/10], Step: [1101/3125], Validation Acc: 53.2\n",
      "Epoch: [8/10], Step: [1201/3125], Validation Acc: 53.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d37eb980b812>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# validate every 100 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "llist=[]\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data1, lengths1,data2, lengths2, labels,id1,id2 in loader:\n",
    "        data_batch1, lengths_batch1,data_batch2, lengths_batch2, label_batch,idd1,idd2 = data1, lengths1,data2, lengths2, labels,id1,id2\n",
    "        outputs = F.softmax(model(data_batch1, lengths_batch1,data_batch2, lengths_batch2,idd1,idd2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "        llist.append([data1,data2,labels,predicted])\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = RNN(emb_size=300, hidden_size=600, num_layers=2, num_classes=3, vocab_size=50002)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data1, lengths1,data2, lengths2, labels,id1,id2) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data1, lengths1,data2, lengths2,id1,id2)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            \n",
    "            val_acc = test_model(val_loader, model) \n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53.322580645161274,\n",
       " 53.551612903225816,\n",
       " 53.125806451612895,\n",
       " 53.529032258064504,\n",
       " 53.58064516129032,\n",
       " 53.57096774193548,\n",
       " 53.53870967741936]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1166780071873819,\n",
       " 1.1196106864560036,\n",
       " 1.134607184317804,\n",
       " 1.1322986041345904,\n",
       " 1.128976425816936,\n",
       " 1.1235411513236262,\n",
       " 1.1283040046691895]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
