{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets improve libraries that we are going to be used in this lab session\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(134)\n",
    "import pandas as pd\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "tt_sen1 = pkl.load(open(\"tt_sen1.p\", \"rb\"))\n",
    "tt_sen2 = pkl.load(open(\"tt_sen2.p\", \"rb\"))\n",
    "tv_sen1 = pkl.load(open(\"tv_sen1.p\", \"rb\"))\n",
    "tv_sen2 = pkl.load(open(\"tv_sen2.p\", \"rb\"))\n",
    "train_targets = pkl.load(open(\"train_targets.p\", \"rb\"))\n",
    "val_targets = pkl.load(open(\"val_targets.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_load = 50000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open('wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i\n",
    "        idx2words_ft[i] = s[0]\n",
    "        ordered_words_ft.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id2token\n",
    "x=idx2words_ft\n",
    "#token2idx\n",
    "y=words_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2words_ft={}\n",
    "idx2words_ft[0]='<pad>'\n",
    "idx2words_ft[1]='<unk>'\n",
    "for i in range(0,50000):\n",
    "    idx2words_ft[i+2]=x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=list(words_ft.keys())\n",
    "words_ft={}\n",
    "words_ft['<pad>']=0\n",
    "words_ft['<unk>']=1\n",
    "for i in range(0,50000):\n",
    "    words_ft[w[i]]=i+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=loaded_embeddings_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings_ft=np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings_ft=np.vstack([loaded_embeddings_ft,np.ones(300)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings_ft=np.vstack([loaded_embeddings_ft,z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id=words_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token=list(idx2words_ft.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 100000\n",
      "Train dataset size is 100000\n",
      "Val dataset size is 1000\n",
      "Val dataset size is 1000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):  #REplaces each token with respective index\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices1 = token2index_dataset(tt_sen1)\n",
    "train_data_indices2 = token2index_dataset(tt_sen2)\n",
    "val_data_indices1 = token2index_dataset(tv_sen1)\n",
    "val_data_indices2 = token2index_dataset(tv_sen2)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices1)))\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices2)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices1)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm=0\n",
    "for i in range(0,100000):\n",
    "    r=max(len(train_data_indices1[i]),len(train_data_indices2[i]))\n",
    "    if mm<r:\n",
    "        mm=r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"    \n",
    "    def __init__(self, data_list1,data_list2, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1 = data_list1\n",
    "        self.data_list2 = data_list2\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list1) == len(self.target_list))\n",
    "        assert (len(self.data_list2) == len(self.target_list))\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "    def __len__(self):\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "        return len(self.data_list1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        token_idx1 = self.data_list1[key][:MAX_SENTENCE_LENGTH]       \n",
    "        token_idx2 = self.data_list2[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx1, len(token_idx1),token_idx2, len(token_idx2), label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = mm\n",
    "#MAX_SENTENCE_LENGTH is a hyperparameter\n",
    "#We implement dataset first before data loader. It takes 2 things as input.\n",
    "#Datatlist (dataset converted to indices of tokens)\n",
    "#Targetlist ( number between 1-20 that represents the target of document)\n",
    "#We need to implement len and getitem\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"    \n",
    "    def __init__(self, data_list1,data_list2, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1 = data_list1\n",
    "        self.data_list2 = data_list2\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list1) == len(self.target_list))\n",
    "        assert (len(self.data_list2) == len(self.target_list))\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "    def __len__(self):\n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "        return len(self.data_list1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        token_idx1 = self.data_list1[key][:MAX_SENTENCE_LENGTH]       \n",
    "        token_idx2 = self.data_list2[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx1, len(token_idx1),token_idx2, len(token_idx2), label]\n",
    "#Collate function adds padding symbols to data in case its smaller than\n",
    "# the max sentence length\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    length_list1 = []\n",
    "    data_list2 = []\n",
    "    length_list2 = []\n",
    "    label_list = []\n",
    "\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list1.append(datum[1])\n",
    "        length_list2.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list1.append(padded_vec1)\n",
    "        padded_vec2 = np.pad(np.array(datum[2]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list2.append(padded_vec2)\n",
    "        \n",
    "    ind_dec_order1 = np.argsort(length_list1)[::-1]\n",
    "    ind_dec_order2 = np.argsort(length_list2)[::-1]\n",
    "    \n",
    "    data_list1 = np.array(data_list1)[ind_dec_order1]\n",
    "    data_list2 = np.array(data_list2)[ind_dec_order2]\n",
    "    \n",
    "    length_list1 = np.array(length_list1)[ind_dec_order1]\n",
    "    length_list2 = np.array(length_list2)[ind_dec_order2]\n",
    "    \n",
    "#     label_list = np.array(label_list)[ind_dec_order]    \n",
    "        \n",
    "    return [torch.from_numpy(np.array(data_list1)), torch.from_numpy(np.array(np.int32(length_list1))) ,torch.from_numpy(np.array(data_list2)), torch.from_numpy(np.array(np.int32(length_list2))), torch.from_numpy(np.array(np.int32(label_list))).long(),ind_dec_order1,ind_dec_order2]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "#train_dataset is a hyperparameter and also batchsize\n",
    "#train and validation also has shuffling here\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices1, train_data_indices2, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices1, val_data_indices2, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets implement basic Convolutional Neural Net model for text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.FloatTensor(loaded_embeddings_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight,freeze=True)\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x1, lengths1, x2, lengths2,id1,id2):\n",
    "        # reset hidden state\n",
    "\n",
    "        batch_size1, seq_len1 = x1.size()\n",
    "        batch_size2, seq_len2 = x2.size()\n",
    "        \n",
    "        self.hidden1 = self.init_hidden(batch_size1)\n",
    "        self.hidden2 = self.init_hidden(batch_size2)\n",
    "        # get embedding of characters\n",
    "        embed1 = self.embedding(x1)\n",
    "        embed2 = self.embedding(x2)\n",
    "        # pack padded sequence\n",
    "        #Pytorch wants sequences to be in descending order. First one have longest length and\n",
    "        #Then the subsequent ones have lesser lengths\n",
    "        embed1 = torch.nn.utils.rnn.pack_padded_sequence(embed1, lengths1.numpy(), batch_first=True)\n",
    "        embed2 = torch.nn.utils.rnn.pack_padded_sequence(embed2, lengths2.numpy(), batch_first=True)\n",
    "        \n",
    "        # fprop though RNN\n",
    "        rnn_out1, self.hidden1 = self.rnn(embed1, self.hidden1)\n",
    "        rnn_out2, self.hidden2 = self.rnn(embed2, self.hidden2)\n",
    "        \n",
    "        # undo packing\n",
    "        rnn_out1, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out1, batch_first=True)\n",
    "        rnn_out2, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out2, batch_first=True)\n",
    "\n",
    "        # sum hidden activations of RNN across time\n",
    "        rnn_out1 = torch.sum(rnn_out1, dim=1)\n",
    "        rnn_out2 = torch.sum(rnn_out2, dim=1)\n",
    "        \n",
    "\n",
    "        r1=rnn_out1.clone()\n",
    "        r2=rnn_out2.clone()\n",
    "        for i in range(0,len(r1)):\n",
    "            rnn_out1[i]=r1[id1[i]]\n",
    "        for i in range(0,len(r2)):\n",
    "            rnn_out2[i]=r2[id2[i]]\n",
    "        rnnoutfinal = torch.mul(rnn_out1, rnn_out2)\n",
    "\n",
    "        logits = self.linear(rnnoutfinal)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 53.0\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 53.1\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 56.4\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 52.3\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 53.3\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 53.6\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 54.2\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 54.3\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 53.9\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 52.8\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 54.0\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 54.4\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 50.6\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 53.7\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 53.7\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 54.9\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 51.3\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 50.7\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 52.2\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 52.5\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 52.4\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 53.3\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 54.3\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 54.0\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 54.6\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 55.2\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 54.3\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 53.4\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 53.6\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 53.3\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 50.7\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 52.8\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 53.1\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 52.6\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 55.1\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 52.2\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 50.8\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 53.5\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 50.1\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 53.7\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 53.2\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 55.6\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 51.4\n",
      "Epoch: [2/10], Step: [1301/3125], Validation Acc: 51.7\n",
      "Epoch: [2/10], Step: [1401/3125], Validation Acc: 53.6\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 52.3\n",
      "Epoch: [2/10], Step: [1601/3125], Validation Acc: 51.4\n",
      "Epoch: [2/10], Step: [1701/3125], Validation Acc: 53.3\n",
      "Epoch: [2/10], Step: [1801/3125], Validation Acc: 53.7\n",
      "Epoch: [2/10], Step: [1901/3125], Validation Acc: 54.6\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 54.0\n",
      "Epoch: [2/10], Step: [2101/3125], Validation Acc: 53.1\n",
      "Epoch: [2/10], Step: [2201/3125], Validation Acc: 53.5\n",
      "Epoch: [2/10], Step: [2301/3125], Validation Acc: 52.1\n",
      "Epoch: [2/10], Step: [2401/3125], Validation Acc: 54.1\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 54.2\n",
      "Epoch: [2/10], Step: [2601/3125], Validation Acc: 52.2\n",
      "Epoch: [2/10], Step: [2701/3125], Validation Acc: 53.5\n",
      "Epoch: [2/10], Step: [2801/3125], Validation Acc: 54.8\n",
      "Epoch: [2/10], Step: [2901/3125], Validation Acc: 54.0\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 53.4\n",
      "Epoch: [2/10], Step: [3101/3125], Validation Acc: 51.6\n",
      "Epoch: [3/10], Step: [101/3125], Validation Acc: 53.8\n",
      "Epoch: [3/10], Step: [201/3125], Validation Acc: 50.5\n",
      "Epoch: [3/10], Step: [301/3125], Validation Acc: 52.0\n",
      "Epoch: [3/10], Step: [401/3125], Validation Acc: 52.9\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 55.3\n",
      "Epoch: [3/10], Step: [601/3125], Validation Acc: 53.8\n",
      "Epoch: [3/10], Step: [701/3125], Validation Acc: 53.5\n",
      "Epoch: [3/10], Step: [801/3125], Validation Acc: 51.9\n",
      "Epoch: [3/10], Step: [901/3125], Validation Acc: 57.7\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 54.0\n",
      "Epoch: [3/10], Step: [1101/3125], Validation Acc: 54.6\n",
      "Epoch: [3/10], Step: [1201/3125], Validation Acc: 54.2\n",
      "Epoch: [3/10], Step: [1301/3125], Validation Acc: 53.1\n",
      "Epoch: [3/10], Step: [1401/3125], Validation Acc: 55.2\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 50.9\n",
      "Epoch: [3/10], Step: [1601/3125], Validation Acc: 51.5\n",
      "Epoch: [3/10], Step: [1701/3125], Validation Acc: 53.7\n",
      "Epoch: [3/10], Step: [1801/3125], Validation Acc: 55.3\n",
      "Epoch: [3/10], Step: [1901/3125], Validation Acc: 53.2\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 54.8\n",
      "Epoch: [3/10], Step: [2101/3125], Validation Acc: 52.7\n",
      "Epoch: [3/10], Step: [2201/3125], Validation Acc: 55.8\n",
      "Epoch: [3/10], Step: [2301/3125], Validation Acc: 51.2\n",
      "Epoch: [3/10], Step: [2401/3125], Validation Acc: 52.2\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 53.8\n",
      "Epoch: [3/10], Step: [2601/3125], Validation Acc: 52.5\n",
      "Epoch: [3/10], Step: [2701/3125], Validation Acc: 54.1\n",
      "Epoch: [3/10], Step: [2801/3125], Validation Acc: 54.6\n",
      "Epoch: [3/10], Step: [2901/3125], Validation Acc: 53.1\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 51.3\n",
      "Epoch: [3/10], Step: [3101/3125], Validation Acc: 54.1\n",
      "Epoch: [4/10], Step: [101/3125], Validation Acc: 54.7\n",
      "Epoch: [4/10], Step: [201/3125], Validation Acc: 53.2\n",
      "Epoch: [4/10], Step: [301/3125], Validation Acc: 54.4\n",
      "Epoch: [4/10], Step: [401/3125], Validation Acc: 55.1\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 53.0\n",
      "Epoch: [4/10], Step: [601/3125], Validation Acc: 52.0\n",
      "Epoch: [4/10], Step: [701/3125], Validation Acc: 53.6\n",
      "Epoch: [4/10], Step: [801/3125], Validation Acc: 52.1\n",
      "Epoch: [4/10], Step: [901/3125], Validation Acc: 53.9\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 53.1\n",
      "Epoch: [4/10], Step: [1101/3125], Validation Acc: 54.5\n",
      "Epoch: [4/10], Step: [1201/3125], Validation Acc: 54.3\n",
      "Epoch: [4/10], Step: [1301/3125], Validation Acc: 54.5\n",
      "Epoch: [4/10], Step: [1401/3125], Validation Acc: 50.3\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 54.1\n",
      "Epoch: [4/10], Step: [1601/3125], Validation Acc: 54.5\n",
      "Epoch: [4/10], Step: [1701/3125], Validation Acc: 52.7\n",
      "Epoch: [4/10], Step: [1801/3125], Validation Acc: 54.5\n",
      "Epoch: [4/10], Step: [1901/3125], Validation Acc: 52.7\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 54.3\n",
      "Epoch: [4/10], Step: [2101/3125], Validation Acc: 51.9\n",
      "Epoch: [4/10], Step: [2201/3125], Validation Acc: 52.6\n",
      "Epoch: [4/10], Step: [2301/3125], Validation Acc: 53.1\n",
      "Epoch: [4/10], Step: [2401/3125], Validation Acc: 51.3\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 52.3\n",
      "Epoch: [4/10], Step: [2601/3125], Validation Acc: 51.7\n",
      "Epoch: [4/10], Step: [2701/3125], Validation Acc: 55.1\n",
      "Epoch: [4/10], Step: [2801/3125], Validation Acc: 53.2\n",
      "Epoch: [4/10], Step: [2901/3125], Validation Acc: 52.6\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 53.8\n",
      "Epoch: [4/10], Step: [3101/3125], Validation Acc: 53.8\n",
      "Epoch: [5/10], Step: [101/3125], Validation Acc: 53.7\n",
      "Epoch: [5/10], Step: [201/3125], Validation Acc: 49.2\n",
      "Epoch: [5/10], Step: [301/3125], Validation Acc: 50.3\n",
      "Epoch: [5/10], Step: [401/3125], Validation Acc: 52.1\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 52.8\n",
      "Epoch: [5/10], Step: [601/3125], Validation Acc: 49.8\n",
      "Epoch: [5/10], Step: [701/3125], Validation Acc: 55.2\n",
      "Epoch: [5/10], Step: [801/3125], Validation Acc: 56.2\n",
      "Epoch: [5/10], Step: [901/3125], Validation Acc: 53.7\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 53.3\n",
      "Epoch: [5/10], Step: [1101/3125], Validation Acc: 55.4\n",
      "Epoch: [5/10], Step: [1201/3125], Validation Acc: 56.1\n",
      "Epoch: [5/10], Step: [1301/3125], Validation Acc: 53.5\n",
      "Epoch: [5/10], Step: [1401/3125], Validation Acc: 51.2\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 51.8\n",
      "Epoch: [5/10], Step: [1601/3125], Validation Acc: 53.4\n",
      "Epoch: [5/10], Step: [1701/3125], Validation Acc: 54.4\n",
      "Epoch: [5/10], Step: [1801/3125], Validation Acc: 53.4\n",
      "Epoch: [5/10], Step: [1901/3125], Validation Acc: 52.9\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 51.5\n",
      "Epoch: [5/10], Step: [2101/3125], Validation Acc: 52.4\n",
      "Epoch: [5/10], Step: [2201/3125], Validation Acc: 52.6\n",
      "Epoch: [5/10], Step: [2301/3125], Validation Acc: 55.1\n",
      "Epoch: [5/10], Step: [2401/3125], Validation Acc: 53.7\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 53.1\n",
      "Epoch: [5/10], Step: [2601/3125], Validation Acc: 50.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/10], Step: [2701/3125], Validation Acc: 53.5\n",
      "Epoch: [5/10], Step: [2801/3125], Validation Acc: 56.0\n",
      "Epoch: [5/10], Step: [2901/3125], Validation Acc: 52.2\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 52.8\n",
      "Epoch: [5/10], Step: [3101/3125], Validation Acc: 54.6\n",
      "Epoch: [6/10], Step: [101/3125], Validation Acc: 52.3\n",
      "Epoch: [6/10], Step: [201/3125], Validation Acc: 55.5\n",
      "Epoch: [6/10], Step: [301/3125], Validation Acc: 55.1\n",
      "Epoch: [6/10], Step: [401/3125], Validation Acc: 51.4\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 52.3\n",
      "Epoch: [6/10], Step: [601/3125], Validation Acc: 55.7\n",
      "Epoch: [6/10], Step: [701/3125], Validation Acc: 53.2\n",
      "Epoch: [6/10], Step: [801/3125], Validation Acc: 51.3\n",
      "Epoch: [6/10], Step: [901/3125], Validation Acc: 54.5\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 51.2\n",
      "Epoch: [6/10], Step: [1101/3125], Validation Acc: 53.1\n",
      "Epoch: [6/10], Step: [1201/3125], Validation Acc: 52.0\n",
      "Epoch: [6/10], Step: [1301/3125], Validation Acc: 50.6\n",
      "Epoch: [6/10], Step: [1401/3125], Validation Acc: 53.2\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 53.4\n",
      "Epoch: [6/10], Step: [1601/3125], Validation Acc: 51.7\n",
      "Epoch: [6/10], Step: [1701/3125], Validation Acc: 53.9\n",
      "Epoch: [6/10], Step: [1801/3125], Validation Acc: 55.3\n",
      "Epoch: [6/10], Step: [1901/3125], Validation Acc: 55.4\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 51.9\n",
      "Epoch: [6/10], Step: [2101/3125], Validation Acc: 50.9\n",
      "Epoch: [6/10], Step: [2201/3125], Validation Acc: 55.1\n",
      "Epoch: [6/10], Step: [2301/3125], Validation Acc: 52.5\n",
      "Epoch: [6/10], Step: [2401/3125], Validation Acc: 52.9\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 51.2\n",
      "Epoch: [6/10], Step: [2601/3125], Validation Acc: 53.9\n",
      "Epoch: [6/10], Step: [2701/3125], Validation Acc: 54.4\n",
      "Epoch: [6/10], Step: [2801/3125], Validation Acc: 54.4\n",
      "Epoch: [6/10], Step: [2901/3125], Validation Acc: 54.1\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 52.0\n",
      "Epoch: [6/10], Step: [3101/3125], Validation Acc: 52.0\n",
      "Epoch: [7/10], Step: [101/3125], Validation Acc: 52.5\n",
      "Epoch: [7/10], Step: [201/3125], Validation Acc: 52.6\n",
      "Epoch: [7/10], Step: [301/3125], Validation Acc: 52.7\n",
      "Epoch: [7/10], Step: [401/3125], Validation Acc: 52.3\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 53.2\n",
      "Epoch: [7/10], Step: [601/3125], Validation Acc: 53.3\n",
      "Epoch: [7/10], Step: [701/3125], Validation Acc: 52.1\n",
      "Epoch: [7/10], Step: [801/3125], Validation Acc: 53.9\n",
      "Epoch: [7/10], Step: [901/3125], Validation Acc: 51.0\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 52.7\n",
      "Epoch: [7/10], Step: [1101/3125], Validation Acc: 53.5\n",
      "Epoch: [7/10], Step: [1201/3125], Validation Acc: 54.2\n",
      "Epoch: [7/10], Step: [1301/3125], Validation Acc: 54.8\n",
      "Epoch: [7/10], Step: [1401/3125], Validation Acc: 53.2\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 51.1\n",
      "Epoch: [7/10], Step: [1601/3125], Validation Acc: 54.5\n",
      "Epoch: [7/10], Step: [1701/3125], Validation Acc: 54.3\n",
      "Epoch: [7/10], Step: [1801/3125], Validation Acc: 53.3\n",
      "Epoch: [7/10], Step: [1901/3125], Validation Acc: 55.3\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 52.8\n",
      "Epoch: [7/10], Step: [2101/3125], Validation Acc: 53.1\n",
      "Epoch: [7/10], Step: [2201/3125], Validation Acc: 53.0\n",
      "Epoch: [7/10], Step: [2301/3125], Validation Acc: 52.8\n",
      "Epoch: [7/10], Step: [2401/3125], Validation Acc: 54.1\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 52.8\n",
      "Epoch: [7/10], Step: [2601/3125], Validation Acc: 55.6\n",
      "Epoch: [7/10], Step: [2701/3125], Validation Acc: 54.0\n",
      "Epoch: [7/10], Step: [2801/3125], Validation Acc: 52.6\n",
      "Epoch: [7/10], Step: [2901/3125], Validation Acc: 52.9\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 52.0\n",
      "Epoch: [7/10], Step: [3101/3125], Validation Acc: 52.0\n",
      "Epoch: [8/10], Step: [101/3125], Validation Acc: 52.4\n",
      "Epoch: [8/10], Step: [201/3125], Validation Acc: 55.3\n",
      "Epoch: [8/10], Step: [301/3125], Validation Acc: 53.1\n",
      "Epoch: [8/10], Step: [401/3125], Validation Acc: 53.7\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 54.7\n",
      "Epoch: [8/10], Step: [601/3125], Validation Acc: 54.6\n",
      "Epoch: [8/10], Step: [701/3125], Validation Acc: 53.9\n",
      "Epoch: [8/10], Step: [801/3125], Validation Acc: 51.6\n",
      "Epoch: [8/10], Step: [901/3125], Validation Acc: 53.6\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 50.1\n",
      "Epoch: [8/10], Step: [1101/3125], Validation Acc: 53.6\n",
      "Epoch: [8/10], Step: [1201/3125], Validation Acc: 51.6\n",
      "Epoch: [8/10], Step: [1301/3125], Validation Acc: 54.3\n",
      "Epoch: [8/10], Step: [1401/3125], Validation Acc: 54.9\n",
      "Epoch: [8/10], Step: [1501/3125], Validation Acc: 53.0\n",
      "Epoch: [8/10], Step: [1601/3125], Validation Acc: 53.6\n",
      "Epoch: [8/10], Step: [1701/3125], Validation Acc: 53.2\n",
      "Epoch: [8/10], Step: [1801/3125], Validation Acc: 54.1\n",
      "Epoch: [8/10], Step: [1901/3125], Validation Acc: 52.5\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 53.6\n",
      "Epoch: [8/10], Step: [2101/3125], Validation Acc: 53.9\n",
      "Epoch: [8/10], Step: [2201/3125], Validation Acc: 53.4\n",
      "Epoch: [8/10], Step: [2301/3125], Validation Acc: 52.8\n",
      "Epoch: [8/10], Step: [2401/3125], Validation Acc: 50.5\n",
      "Epoch: [8/10], Step: [2501/3125], Validation Acc: 53.8\n",
      "Epoch: [8/10], Step: [2601/3125], Validation Acc: 55.4\n",
      "Epoch: [8/10], Step: [2701/3125], Validation Acc: 54.2\n",
      "Epoch: [8/10], Step: [2801/3125], Validation Acc: 55.5\n",
      "Epoch: [8/10], Step: [2901/3125], Validation Acc: 53.4\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 53.9\n",
      "Epoch: [8/10], Step: [3101/3125], Validation Acc: 49.6\n",
      "Epoch: [9/10], Step: [101/3125], Validation Acc: 55.2\n",
      "Epoch: [9/10], Step: [201/3125], Validation Acc: 51.5\n",
      "Epoch: [9/10], Step: [301/3125], Validation Acc: 55.2\n",
      "Epoch: [9/10], Step: [401/3125], Validation Acc: 54.2\n",
      "Epoch: [9/10], Step: [501/3125], Validation Acc: 53.3\n",
      "Epoch: [9/10], Step: [601/3125], Validation Acc: 52.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-cde1f3f176e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-34d13021c3a7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, lengths1, x2, lengths2, id1, id2)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# fprop though RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mrnn_out1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mrnn_out2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# undo packing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvariable_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mflat_hidden\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mRNNTanhCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mRNNTanhCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mhy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pl=[]\n",
    "pa=[]\n",
    "epl=[]\n",
    "epa=[]\n",
    "llist=[]\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data1, lengths1,data2, lengths2, labels,id1,id2 in loader:\n",
    "        data_batch1, lengths_batch1,data_batch2, lengths_batch2, label_batch,idd1,idd2 = data1, lengths1,data2, lengths2, labels,id1,id2\n",
    "        outputs = F.softmax(model(data_batch1, lengths_batch1,data_batch2, lengths_batch2,idd1,idd2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "        llist.append([data1,data2,labels,predicted])\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = RNN(emb_size=300, hidden_size=500, num_layers=2, num_classes=3, vocab_size=50002)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    l=0\n",
    "    a=0\n",
    "    c=0\n",
    "    for i, (data1, lengths1,data2, lengths2, labels,id1,id2) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data1, lengths1,data2, lengths2,id1,id2)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            \n",
    "            val_acc = test_model(val_loader, model) \n",
    "            c=c+1\n",
    "            l=l+loss.item()\n",
    "            a=a+val_acc\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            pa.append(val_acc)\n",
    "            pl.append(loss.item())\n",
    "    epl.append(l/c)\n",
    "    epa.append(a/c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53.354838709677416,\n",
       " 53.070967741935476,\n",
       " 53.46129032258064,\n",
       " 53.319354838709664,\n",
       " 53.09354838709676,\n",
       " 53.109677419354846,\n",
       " 53.16774193548386,\n",
       " 53.3483870967742]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1082172163071171,\n",
       " 1.105687564419162,\n",
       " 1.1171741639414141,\n",
       " 1.1088292483360536,\n",
       " 1.1070284074352634,\n",
       " 1.109632549747344,\n",
       " 1.1041128827679543,\n",
       " 1.1018894257084015]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
